{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbaab93-9a09-4926-b3bd-0d0ff50b3b59",
   "metadata": {},
   "source": [
    "# Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9453d2-12cd-4f28-9588-4b6293ef20ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./see-segment')\n",
    "\n",
    "from SEE_hug import list_of_models\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec4c6c5-fa40-49ea-83ee-883bdb36d6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = list_of_models.fetch_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61bb3f2-0f18-42f4-bdcb-c3ff9ee53180",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = list_of_models.get_imagefiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32287508-a660-48fd-9ec9-56136aa28ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model 0 millionhz/segformer-b0-finetuned-flame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ millionhz/segformer-b0-finetuned-flame\n",
      "Checking model 1 bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS ERROR: bilal01/segformer-b0-finetuned-segments-sidewalk-2 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/bilal01/segformer-b0-finetuned-segments-sidewalk-2/main' for available files. @ bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 2 openmmlab/upernet-convnext-large\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-convnext-large\n",
      "Checking model 3 harshm121/M3L\n",
      "#FITNESS ERROR: harshm121/M3L does not appear to have a file named config.json. Checkout 'https://huggingface.co/harshm121/M3L/main' for available files. @ harshm121/M3L\n",
      "Checking model 4 andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "#FITNESS [1.5, 2, 2] @ andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "Checking model 5 google/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [0.08439344618055555, 2, 2] @ google/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 6 imageomics/BGNN-trait-segmentation\n",
      "#FITNESS ERROR: imageomics/BGNN-trait-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/imageomics/BGNN-trait-segmentation/main' for available files. @ imageomics/BGNN-trait-segmentation\n",
      "Checking model 7 hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 8 nielsr/segformer-test-v6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-test-v6\n",
      "Checking model 9 keremberke/yolov8m-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pothole-segmentation\n",
      "Checking model 10 facebook/mask2former-swin-large-mapillary-vistas-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-large-mapillary-vistas-semantic\n",
      "Checking model 11 shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "Checking model 12 NimaBoscarino/IS-Net_DIS-general-use\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS-general-use does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS-general-use/main' for available files. @ NimaBoscarino/IS-Net_DIS-general-use\n",
      "Checking model 13 nielsr/segformer-trainer-test-bis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-trainer-test-bis\n",
      "Checking model 14 Efferbach/segformer-finetuned-lane-10k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ Efferbach/segformer-finetuned-lane-10k-steps\n",
      "Checking model 15 facebook/mask2former-swin-base-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-coco-instance\n",
      "Checking model 16 shi-labs/oneformer_ade20k_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_ade20k_dinat_large\n",
      "Checking model 17 Sadiksmart0/unet\n",
      "#FITNESS ERROR: Sadiksmart0/unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sadiksmart0/unet/main' for available files. @ Sadiksmart0/unet\n",
      "Checking model 18 openmmlab/upernet-swin-base\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-swin-base\n",
      "Checking model 19 koushikn/segformer-finetuned-Maize-10k-steps-sem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ koushikn/segformer-finetuned-Maize-10k-steps-sem\n",
      "Checking model 20 facebook/mask2former-swin-large-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-large-cityscapes-panoptic\n",
      "Checking model 21 nielsr/sidewalk-semantic-demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/sidewalk-semantic-demo\n",
      "Checking model 22 fabda/nucount\n",
      "#FITNESS ERROR: Unrecognized model in fabda/nucount. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ fabda/nucount\n",
      "Checking model 23 nvidia/segformer-b0-finetuned-cityscapes-640-1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b0-finetuned-cityscapes-640-1280\n",
      "Checking model 24 facebook/mask2former-swin-tiny-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-cityscapes-instance\n",
      "Checking model 25 nielsr/segformer-b0-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 26 microsoft/beit-base-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ microsoft/beit-base-finetuned-ade-640-640\n",
      "Checking model 27 shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "Checking model 28 nvidia/segformer-b3-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b3-finetuned-ade-512-512\n",
      "Checking model 29 keremberke/yolov8n-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pcb-defect-segmentation\n",
      "Checking model 30 kiheh85202/yolo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at kiheh85202/yolo and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ kiheh85202/yolo\n",
      "Checking model 31 skaliy/spine-segmentation\n",
      "#FITNESS ERROR: skaliy/spine-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/spine-segmentation/main' for available files. @ skaliy/spine-segmentation\n",
      "Checking model 32 Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [0.08439344618055555, 2, 2] @ Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 33 lmazzon70/deeplab-v3\n",
      "#FITNESS ERROR: lmazzon70/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/lmazzon70/deeplab-v3/main' for available files. @ lmazzon70/deeplab-v3\n",
      "Checking model 34 NimaBoscarino/IS-Net_DIS\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS/main' for available files. @ NimaBoscarino/IS-Net_DIS\n",
      "Checking model 35 keras-io/deeplabv3p-resnet50\n",
      "#FITNESS ERROR: keras-io/deeplabv3p-resnet50 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/deeplabv3p-resnet50/main' for available files. @ keras-io/deeplabv3p-resnet50\n",
      "Checking model 36 shi-labs/oneformer_ade20k_swin_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/transformers/models/oneformer/image_processing_oneformer.py:417: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ shi-labs/oneformer_ade20k_swin_tiny\n",
      "Checking model 37 keremberke/yolov8n-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-building-segmentation\n",
      "Checking model 38 facebook/mask2former-swin-base-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-base-coco-panoptic\n",
      "Checking model 39 facebook/mask2former-swin-tiny-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.08118598090277777, 2, 2] @ facebook/mask2former-swin-tiny-cityscapes-semantic\n",
      "Checking model 40 Ragweed/sidewalk-segmentation\n",
      "#FITNESS ERROR: Ragweed/sidewalk-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/Ragweed/sidewalk-segmentation/main' for available files. @ Ragweed/sidewalk-segmentation\n",
      "Checking model 41 facebook/mask2former-swin-large-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-cityscapes-instance\n",
      "Checking model 42 apple/deeplabv3-mobilevit-x-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.060430772569444444, 2, 2] @ apple/deeplabv3-mobilevit-x-small\n",
      "Checking model 43 shi-labs/oneformer_ade20k_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ shi-labs/oneformer_ade20k_swin_large\n",
      "Checking model 44 Efferbach/mobilevit-small-10k-steps\n",
      "#FITNESS [1.5, 2, 1] @ Efferbach/mobilevit-small-10k-steps\n",
      "Checking model 45 zklee98/segformer-b1-solarModuleAnomaly-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ zklee98/segformer-b1-solarModuleAnomaly-v0.1\n",
      "Checking model 46 bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "#FITNESS [1.5, 2, 1] @ bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "Checking model 47 OpenGVLab/PATH-ViTB\n",
      "#FITNESS ERROR: OpenGVLab/PATH-ViTB does not appear to have a file named config.json. Checkout 'https://huggingface.co/OpenGVLab/PATH-ViTB/main' for available files. @ OpenGVLab/PATH-ViTB\n",
      "Checking model 48 matjesg/deepflash2_demo\n",
      "#FITNESS ERROR: matjesg/deepflash2_demo does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/deepflash2_demo/main' for available files. @ matjesg/deepflash2_demo\n",
      "Checking model 49 AlmogM/segformer-b0-finetuned-fish-almogm\n",
      "#FITNESS [0.0565234375, 2, 2] @ AlmogM/segformer-b0-finetuned-fish-almogm\n",
      "Checking model 50 nvidia/segformer-b0-finetuned-cityscapes-512-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b0-finetuned-cityscapes-512-1024\n",
      "Checking model 51 facebook/maskformer-swin-tiny-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/transformers/models/maskformer/image_processing_maskformer.py:401: FutureWarning: The `size_divisibility` argument is deprecated and will be removed in v4.27. Please use `size_divisor` instead.\n",
      "  warnings.warn(\n",
      "/Users/ishasharma/anaconda3/lib/python3.10/site-packages/transformers/models/maskformer/image_processing_maskformer.py:408: FutureWarning: The `max_size` argument is deprecated and will be removed in v4.27. Please use size['longest_edge'] instead.\n",
      "  warnings.warn(\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-tiny-ade\n",
      "Checking model 52 matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 2, 2] @ matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 53 yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "#FITNESS [1.5, 2, 2] @ yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "Checking model 54 Onegafer/segformer-v-mesh-0\n",
      "#FITNESS [1.5, 2, 2] @ Onegafer/segformer-v-mesh-0\n",
      "Checking model 55 keremberke/yolov8m-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-building-segmentation\n",
      "Checking model 56 edwardhuang/test-carbonate-segmentation2\n",
      "#FITNESS [1.5, 2, 2] @ edwardhuang/test-carbonate-segmentation2\n",
      "Checking model 57 Edalik/hockey\n",
      "#FITNESS ERROR: Unrecognized model in Edalik/hockey. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Edalik/hockey\n",
      "Checking model 58 Leiyao-Cui/STRAP\n",
      "#FITNESS ERROR: Leiyao-Cui/STRAP does not appear to have a file named config.json. Checkout 'https://huggingface.co/Leiyao-Cui/STRAP/main' for available files. @ Leiyao-Cui/STRAP\n",
      "Checking model 59 openmmlab/upernet-swin-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-swin-large\n",
      "Checking model 60 facebook/mask2former-swin-tiny-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-tiny-coco-panoptic\n",
      "Checking model 61 nvidia/segformer-b4-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b4-finetuned-ade-512-512\n",
      "Checking model 62 prem-timsina/segformer-b0-finetuned-food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.04786024305555556, 2, 2] @ prem-timsina/segformer-b0-finetuned-food\n",
      "Checking model 63 facebook/maskformer-swin-large-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-large-coco\n",
      "Checking model 64 zho/segformer-finetuned-sidewalk-10k-steps\n",
      "#FITNESS [1.5, 2, 2] @ zho/segformer-finetuned-sidewalk-10k-steps\n",
      "Checking model 65 DiTo97/binarization-segformer-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ DiTo97/binarization-segformer-b3\n",
      "Checking model 66 facebook/mask2former-swin-base-IN21k-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-IN21k-coco-instance\n",
      "Checking model 67 nielsr/segformer-test-v7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-test-v7\n",
      "Checking model 68 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "#FITNESS [0.03941297743055556, 2, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "Checking model 69 vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n",
      "Checking model 70 facebook/maskformer-swin-base-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-base-ade\n",
      "Checking model 71 ArturR01/segformer-b0-example-pytorch-blog\n",
      "#FITNESS [1.5, 2, 2] @ ArturR01/segformer-b0-example-pytorch-blog\n",
      "Checking model 72 mraottth/trashbot\n",
      "#FITNESS [1.5, 2, 1] @ mraottth/trashbot\n",
      "Checking model 73 jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "Checking model 74 Lewislou/cell-seg-sribd\n",
      "#FITNESS ERROR: Lewislou/cell-seg-sribd does not appear to have a file named config.json. Checkout 'https://huggingface.co/Lewislou/cell-seg-sribd/main' for available files. @ Lewislou/cell-seg-sribd\n",
      "Checking model 75 keras-io/semantic-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in keras-io/semantic-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ keras-io/semantic-segmentation\n",
      "Checking model 76 shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "#FITNESS ERROR: Unrecognized image processor in shehan97/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 77 shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "Checking model 78 yasinelh/retinal_vessel_U-Net\n",
      "#FITNESS ERROR: yasinelh/retinal_vessel_U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/yasinelh/retinal_vessel_U-Net/main' for available files. @ yasinelh/retinal_vessel_U-Net\n",
      "Checking model 79 matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "#FITNESS ERROR: matei-dorian/segformer-b0-finetuned-human-parsing does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/matei-dorian/segformer-b0-finetuned-human-parsing/main' for available files. @ matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "Checking model 80 openmmlab/upernet-convnext-small\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-convnext-small\n",
      "Checking model 81 deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "#FITNESS ERROR: deprem-ml/deprem-keras-satellite-semantic-mapping does not appear to have a file named config.json. Checkout 'https://huggingface.co/deprem-ml/deprem-keras-satellite-semantic-mapping/main' for available files. @ deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "Checking model 82 nvidia/segformer-b1-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b1-finetuned-ade-512-512\n",
      "Checking model 83 nvidia/segformer-b0-finetuned-cityscapes-768-768\n",
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b0-finetuned-cityscapes-768-768\n",
      "Checking model 84 merve/deeplab-v3\n",
      "#FITNESS ERROR: merve/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/merve/deeplab-v3/main' for available files. @ merve/deeplab-v3\n",
      "Checking model 85 facebook/mask2former-swin-small-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-small-cityscapes-semantic\n",
      "Checking model 86 facebook/mask2former-swin-large-mapillary-vistas-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-large-mapillary-vistas-panoptic\n",
      "Checking model 87 shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "Checking model 88 SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "#FITNESS ERROR: SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net/main' for available files. @ SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "Checking model 89 s3nh/SegFormer-b4-person-segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b4-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b4-person-segmentation/main' for available files. @ s3nh/SegFormer-b4-person-segmentation\n",
      "Checking model 90 imadd/segformer-b0-finetuned-segments-water-2\n",
      "#FITNESS [1.5, 2, 2] @ imadd/segformer-b0-finetuned-segments-water-2\n",
      "Checking model 91 Andreas-w/brain-classification\n",
      "#FITNESS ERROR: Andreas-w/brain-classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Andreas-w/brain-classification\n",
      "Checking model 92 nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n",
      "Checking model 93 jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "#FITNESS [1.5, 2, 2] @ jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "Checking model 94 s3nh/SegFormer-b0-person-segmentation\n",
      "#FITNESS ERROR: s3nh/SegFormer-b0-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b0-person-segmentation/main' for available files. @ s3nh/SegFormer-b0-person-segmentation\n",
      "Checking model 95 keremberke/yolov8s-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pothole-segmentation\n",
      "Checking model 96 hf-tiny-model-private/tiny-random-DetrForSegmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ hf-tiny-model-private/tiny-random-DetrForSegmentation\n",
      "Checking model 97 openmmlab/upernet-convnext-xlarge\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-convnext-xlarge\n",
      "Checking model 98 andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.083955078125, 2, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n",
      "Checking model 99 jtsang4/test-model\n",
      "#FITNESS ERROR: jtsang4/test-model does not appear to have a file named config.json. Checkout 'https://huggingface.co/jtsang4/test-model/main' for available files. @ jtsang4/test-model\n",
      "Checking model 100 facebook/mask2former-swin-base-IN21k-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-base-IN21k-ade-semantic\n",
      "Checking model 101 turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 2, 1] @ turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 102 nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n",
      "Checking model 103 leonelhs/faceparser\n",
      "#FITNESS ERROR: Unrecognized model in leonelhs/faceparser. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ leonelhs/faceparser\n",
      "Checking model 104 mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 2, 2] @ mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 105 magicmirror/segformer-b4-finetuned-segments-torso\n",
      "#FITNESS ERROR: magicmirror/segformer-b4-finetuned-segments-torso is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ magicmirror/segformer-b4-finetuned-segments-torso\n",
      "Checking model 106 nvidia/segformer-b2-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b2-finetuned-ade-512-512\n",
      "Checking model 107 Saurabh1105/MMDet\n",
      "#FITNESS ERROR: Saurabh1105/MMDet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Saurabh1105/MMDet/main' for available files. @ Saurabh1105/MMDet\n",
      "Checking model 108 optimum/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model optimum/segformer-b0-finetuned-ade-512-512 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ optimum/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 109 shi-labs/oneformer_coco_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ shi-labs/oneformer_coco_swin_large\n",
      "Checking model 110 facebook/maskformer-swin-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-large-ade\n",
      "Checking model 111 apple/deeplabv3-mobilevit-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ apple/deeplabv3-mobilevit-small\n",
      "Checking model 112 Plsek/CADET-v1\n",
      "#FITNESS ERROR: Plsek/CADET-v1 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Plsek/CADET-v1/main' for available files. @ Plsek/CADET-v1\n",
      "Checking model 113 Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: Robindboer/segformer-b5-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "Checking model 114 facebook/mask2former-swin-tiny-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-tiny-ade-semantic\n",
      "Checking model 115 shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "Checking model 116 facebook/mask2former-swin-large-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-large-coco-panoptic\n",
      "Checking model 117 CIDAS/clipseg-rd16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd16\n",
      "Checking model 118 segments-tobias/segformer-b3-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ segments-tobias/segformer-b3-finetuned-segments-sidewalk\n",
      "Checking model 119 apple/mobilevitv2-1.0-voc-deeplabv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Unrecognized image processor in apple/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ apple/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 120 nielsr/segformer-test-sidewalk-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-test-sidewalk-v2\n",
      "Checking model 121 facebook/maskformer-swin-small-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-small-coco\n",
      "Checking model 122 Intel/dpt-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ Intel/dpt-large-ade\n",
      "Checking model 123 mattmdjaga/segformer_b2_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.07010091145833333, 2, 2] @ mattmdjaga/segformer_b2_clothes\n",
      "Checking model 124 malra/segformer-b5-segments-warehouse1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ malra/segformer-b5-segments-warehouse1\n",
      "Checking model 125 Matthijs/deeplabv3-mobilevit-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Matthijs/deeplabv3-mobilevit-small were not used when initializing MobileViTForSemanticSegmentation: ['mobilevit.encoder.layer.4.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.weight', 'seg_head.classifier.classifier.conv.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.layernorm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.conv.weight', 'seg_head.aspp.convs.2.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.num_batches_tracked', 'seg_head.aspp.project.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.weight', 'seg_head.aspp.convs.0.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.weight', 'seg_head.aspp.convs.1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_mean', 'seg_head.aspp.convs.0.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.2.norm.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.bias', 'seg_head.aspp.convs.3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.weight', 'seg_head.aspp.convs.3.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.layernorm.weight', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.bias', 'seg_head.aspp.project.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.bias', 'mobilevit.conv_stem.norm.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.4.1.fusion.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.weight', 'mobilevit.conv_stem.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_mean', 'seg_head.aspp.convs.2.norm.running_mean', 'seg_head.aspp.project.norm.bias', 'seg_head.aspp.project.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.conv.weight', 'seg_head.aspp.convs.0.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.num_batches_tracked', 'seg_head.aspp.convs.1.norm.running_mean', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.bias', 'seg_head.aspp.convs.3.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.fusion.norm.bias', 'mobilevit.conv_stem.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.bias', 'seg_head.aspp.convs.3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.conv.weight', 'seg_head.aspp.convs.0.norm.running_var', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.weight', 'seg_head.aspp.convs.3.norm.running_var', 'mobilevit.encoder.layer.3.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.weight', 'seg_head.aspp.project.norm.weight', 'mobilevit.encoder.layer.4.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_mean', 'seg_head.aspp.project.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.weight', 'seg_head.aspp.convs.2.norm.bias', 'seg_head.aspp.convs.1.norm.running_var', 'mobilevit.encoder.layer.2.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.bias', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.weight', 'seg_head.aspp.convs.0.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_var', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.1.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.4.0.reduce_1x1.conv.weight', 'seg_head.aspp.convs.1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_var', 'mobilevit.conv_stem.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.num_batches_tracked', 'seg_head.classifier.classifier.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_mean', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_var', 'mobilevit.conv_stem.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_var', 'seg_head.aspp.convs.2.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.weight', 'seg_head.aspp.convs.1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.1.0.conv_3x3.conv.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.fusion.conv.weight', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.bias', 'seg_head.aspp.convs.0.conv.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.weight', 'seg_head.aspp.convs.2.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.weight', 'mobilevit.conv_stem.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.weight']\n",
      "- This IS expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForSemanticSegmentation were not initialized from the model checkpoint at Matthijs/deeplabv3-mobilevit-small and are newly initialized: ['mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.convs.3.normalization.bias', 'mobilevit.encoder.layer.3.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.weight', 'segmentation_head.aspp.convs.2.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.2.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.2.fusion.normalization.weight', 'segmentation_head.aspp.convs.2.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.4.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_projection.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.fusion.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.convolution.weight', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.4.fusion.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.weight', 'mobilevit.encoder.layer.4.fusion.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.aspp.project.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.weight', 'mobilevit.conv_stem.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.fusion.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.4.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.3.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.weight', 'mobilevit.conv_stem.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.bias', 'segmentation_head.aspp.convs.1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.weight', 'segmentation_head.aspp.convs.1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.weight', 'segmentation_head.aspp.convs.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_var', 'segmentation_head.classifier.convolution.bias', 'segmentation_head.aspp.convs.0.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.bias', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.convolution.weight', 'mobilevit.conv_stem.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.conv_stem.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.bias', 'segmentation_head.aspp.convs.0.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.layernorm.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_mean', 'segmentation_head.aspp.convs.1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.weight', 'mobilevit.conv_stem.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.fusion.normalization.running_mean', 'segmentation_head.aspp.project.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.3.fusion.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.classifier.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.project.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_var', 'segmentation_head.aspp.convs.2.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.weight', 'segmentation_head.aspp.convs.0.convolution.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_var', 'segmentation_head.aspp.convs.1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.0.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.4.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.4.conv_projection.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.2.conv_projection.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_kxk.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.1.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.weight', 'segmentation_head.aspp.convs.0.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.weight', 'segmentation_head.aspp.convs.0.normalization.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.bias', 'segmentation_head.aspp.convs.2.normalization.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.3.fusion.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.layernorm.bias', 'segmentation_head.aspp.convs.3.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.bias', 'mobilevit.conv_stem.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.convolution.weight', 'segmentation_head.aspp.project.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.bias', 'segmentation_head.aspp.convs.2.normalization.bias', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.4.conv_projection.normalization.weight', 'mobilevit.encoder.layer.2.fusion.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_kxk.convolution.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ Matthijs/deeplabv3-mobilevit-small\n",
      "Checking model 126 zoheb/mit-b5-finetuned-sidewalk-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ zoheb/mit-b5-finetuned-sidewalk-semantic\n",
      "Checking model 127 shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "Checking model 128 facebook/mask2former-swin-small-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-small-coco-instance\n",
      "Checking model 129 shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 2, 2] @ shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 130 skaliy/endometrical_cancer_segmentation\n",
      "#FITNESS ERROR: skaliy/endometrical_cancer_segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/endometrical_cancer_segmentation/main' for available files. @ skaliy/endometrical_cancer_segmentation\n",
      "Checking model 131 Azarthehulk/Image_preprocessing_basics\n",
      "#FITNESS ERROR: Azarthehulk/Image_preprocessing_basics does not appear to have a file named config.json. Checkout 'https://huggingface.co/Azarthehulk/Image_preprocessing_basics/main' for available files. @ Azarthehulk/Image_preprocessing_basics\n",
      "Checking model 132 facebook/mask2former-swin-small-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-small-coco-panoptic\n",
      "Checking model 133 facebook/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/detr-resnet-50-panoptic\n",
      "Checking model 134 shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "Checking model 135 s3nh/SegFormer-b5-person-segm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b5-person-segm does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b5-person-segm/main' for available files. @ s3nh/SegFormer-b5-person-segm\n",
      "Checking model 136 facebook/mask2former-swin-large-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-large-cityscapes-semantic\n",
      "Checking model 137 Robindboer/imageseg\n",
      "#FITNESS ERROR: Robindboer/imageseg does not appear to have a file named config.json. Checkout 'https://huggingface.co/Robindboer/imageseg/main' for available files. @ Robindboer/imageseg\n",
      "Checking model 138 facebook/mask2former-swin-base-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-base-ade-semantic\n",
      "Checking model 139 sayakpaul/mit-b0-finetuned-sidewalk-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-sidewalk-semantic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-sidewalk-semantic\n",
      "Checking model 140 chainyo/segformer-sidewalk\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eb25b12927453ba2e148fbca9caf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/14.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ chainyo/segformer-sidewalk\n",
      "Checking model 141 facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic were not used when initializing Mask2FormerForUniversalSegmentation: ['model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.model.layernorm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.0.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.3.bias', 'model.pixel_level_module.encoder.hidden_states_norms.3.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.embeddings.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.0.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.model.layernorm.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.model.embeddings.norm.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.bias']\n",
      "- This IS expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic and are newly initialized: ['model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n",
      "Checking model 142 thiagohersan/maskformer-satellite-trees\n",
      "#FITNESS ERROR: thiagohersan/maskformer-satellite-trees is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ thiagohersan/maskformer-satellite-trees\n",
      "Checking model 143 shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "Checking model 144 Efferbach/segformer-finetuned-lane-1k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ Efferbach/segformer-finetuned-lane-1k-steps\n",
      "Checking model 145 irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n",
      "Checking model 146 chainyo/segformer-b1-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ chainyo/segformer-b1-sidewalk\n",
      "Checking model 147 facebook/mask2former-swin-base-IN21k-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-IN21k-cityscapes-instance\n",
      "Checking model 148 facebook/mask2former-swin-large-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-coco-instance\n",
      "Checking model 149 facebook/detr-resnet-101-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/detr-resnet-101-panoptic\n",
      "Checking model 150 nielsr/segformer-test-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-test-v5\n",
      "Checking model 151 openmmlab/upernet-convnext-tiny\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-convnext-tiny\n",
      "Checking model 152 nielsr/segformer-trainer-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-trainer-test\n",
      "Checking model 153 sayakpaul/mit-b0-finetuned-pets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-pets with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-pets\n",
      "Checking model 154 thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 155 cvappbakkers/techniparts\n",
      "#FITNESS ERROR: cvappbakkers/techniparts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ cvappbakkers/techniparts\n",
      "Checking model 156 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "#FITNESS [0.03910047743055556, 2, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "Checking model 157 Abhilashvj/clipseg-rd64-refined-copy\n",
      "#FITNESS ERROR: Abhilashvj/clipseg-rd64-refined-copy does not appear to have a file named config.json. Checkout 'https://huggingface.co/Abhilashvj/clipseg-rd64-refined-copy/main' for available files. @ Abhilashvj/clipseg-rd64-refined-copy\n",
      "Checking model 158 Xpitfire/segformer-finetuned-segments-cmp-facade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ Xpitfire/segformer-finetuned-segments-cmp-facade\n",
      "Checking model 159 nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n",
      "Checking model 160 CIDAS/clipseg-rd64-refined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64-refined\n",
      "Checking model 161 alanoix/segformer_b0_flair_one\n",
      "#FITNESS ERROR: alanoix/segformer_b0_flair_one does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/alanoix/segformer_b0_flair_one/main' for available files. @ alanoix/segformer_b0_flair_one\n",
      "Checking model 162 bilal01/segformer-b0-finetuned-segments-test\n",
      "#FITNESS [1.5, 2, 2] @ bilal01/segformer-b0-finetuned-segments-test\n",
      "Checking model 163 Xenova/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model Xenova/detr-resnet-50-panoptic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.detr.modeling_detr.DetrForSegmentation'>). @ Xenova/detr-resnet-50-panoptic\n",
      "Checking model 164 keremberke/yolov8m-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pcb-defect-segmentation\n",
      "Checking model 165 Efferbach/mobilenet_v2_1-10k-steps\n",
      "#FITNESS [1.5, 2, 1] @ Efferbach/mobilenet_v2_1-10k-steps\n",
      "Checking model 166 jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b3-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "Checking model 167 matjesg/cFOS_in_HC\n",
      "#FITNESS ERROR: matjesg/cFOS_in_HC does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/cFOS_in_HC/main' for available files. @ matjesg/cFOS_in_HC\n",
      "Checking model 168 fcakyon/test-model\n",
      "#FITNESS ERROR: 'v8' @ fcakyon/test-model\n",
      "Checking model 169 androks/rembg\n",
      "#FITNESS ERROR: androks/rembg does not appear to have a file named config.json. Checkout 'https://huggingface.co/androks/rembg/main' for available files. @ androks/rembg\n",
      "Checking model 170 facebook/mask2former-swin-tiny-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-tiny-cityscapes-panoptic\n",
      "Checking model 171 openmmlab/upernet-convnext-base\n",
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-convnext-base\n",
      "Checking model 172 segments-tobias/segformer-b0-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ segments-tobias/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 173 q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "#FITNESS [1.5, 2, 1] @ q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "Checking model 174 nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n",
      "Checking model 175 jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "Checking model 176 Narsil/pet-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in Narsil/pet-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Narsil/pet-segmentation\n",
      "Checking model 177 jakka/segformer-b0-finetuned-warehouse-part-1-V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ jakka/segformer-b0-finetuned-warehouse-part-1-V2\n",
      "Checking model 178 facebook/maskformer-swin-tiny-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-tiny-coco\n",
      "Checking model 179 Mendel192/san\n",
      "#FITNESS ERROR: Mendel192/san does not appear to have a file named config.json. Checkout 'https://huggingface.co/Mendel192/san/main' for available files. @ Mendel192/san\n",
      "Checking model 180 facebook/mask2former-swin-small-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-small-cityscapes-panoptic\n",
      "Checking model 181 facebook/mask2former-swin-tiny-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-tiny-coco-instance\n",
      "Checking model 182 nvidia/segformer-b5-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b5-finetuned-ade-640-640\n",
      "Checking model 183 openmmlab/upernet-swin-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-swin-small\n",
      "Checking model 184 shi-labs/oneformer_cityscapes_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_cityscapes_dinat_large\n",
      "Checking model 185 keras-io/monocular-depth-estimation\n",
      "#FITNESS ERROR: keras-io/monocular-depth-estimation does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/monocular-depth-estimation/main' for available files. @ keras-io/monocular-depth-estimation\n",
      "Checking model 186 mraottth/trashbot_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ mraottth/trashbot_v1\n",
      "Checking model 187 yahaoh/ddh-maskrcnn\n",
      "#FITNESS ERROR: yahaoh/ddh-maskrcnn does not appear to have a file named config.json. Checkout 'https://huggingface.co/yahaoh/ddh-maskrcnn/main' for available files. @ yahaoh/ddh-maskrcnn\n",
      "Checking model 188 shi-labs/oneformer_coco_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_coco_dinat_large\n",
      "Checking model 189 nielsr/segformer-finetuned-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-finetuned-sidewalk\n",
      "Checking model 190 shi-labs/oneformer_cityscapes_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ shi-labs/oneformer_cityscapes_swin_large\n",
      "Checking model 191 reannayang/segformer-b0-pavement\n",
      "#FITNESS [1.5, 2, 2] @ reannayang/segformer-b0-pavement\n",
      "Checking model 192 maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "#FITNESS ERROR: maryann-gitonga/brain-tumor-segmentation-3d-attention-unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/maryann-gitonga/brain-tumor-segmentation-3d-attention-unet/main' for available files. @ maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "Checking model 193 nvidia/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 194 jonathandinu/face-parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.026954210069444446, 2, 2] @ jonathandinu/face-parsing\n",
      "Checking model 195 keremberke/yolov8n-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pothole-segmentation\n",
      "Checking model 196 ksahn/pid-s\n",
      "#FITNESS ERROR: ksahn/pid-s is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ ksahn/pid-s\n",
      "Checking model 197 apple/deeplabv3-mobilevit-xx-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ apple/deeplabv3-mobilevit-xx-small\n",
      "Checking model 198 nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n",
      "Checking model 199 facebook/mask2former-swin-small-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-small-cityscapes-instance\n",
      "Checking model 200 facebook/detr-resnet-50-dc5-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/detr-resnet-50-dc5-panoptic\n",
      "Checking model 201 keja/deeplab-v3\n",
      "#FITNESS ERROR: keja/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keja/deeplab-v3/main' for available files. @ keja/deeplab-v3\n",
      "Checking model 202 Sevenlee/kkk\n",
      "#FITNESS ERROR: Sevenlee/kkk does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sevenlee/kkk/main' for available files. @ Sevenlee/kkk\n",
      "Checking model 203 jakka/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ jakka/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 204 Niceforo/teste\n",
      "#FITNESS ERROR: Niceforo/teste does not appear to have a file named config.json. Checkout 'https://huggingface.co/Niceforo/teste/main' for available files. @ Niceforo/teste\n",
      "Checking model 205 mattmdjaga/segformer_b0_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.03680447048611111, 2, 2] @ mattmdjaga/segformer_b0_clothes\n",
      "Checking model 206 cosmobaby/ka\n",
      "#FITNESS ERROR: cosmobaby/ka does not appear to have a file named config.json. Checkout 'https://huggingface.co/cosmobaby/ka/main' for available files. @ cosmobaby/ka\n",
      "Checking model 207 CIDAS/clipseg-rd64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64\n",
      "Checking model 208 voitl/unet_plus_plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: No module named 'segmentation_models_pytorch' @ voitl/unet_plus_plus\n",
      "Checking model 209 facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n",
      "Checking model 210 Lewislou/cellseg_sribd\n",
      "#FITNESS ERROR: 'cell_sribd' @ Lewislou/cellseg_sribd\n",
      "Checking model 211 malra/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ malra/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 212 facebook/mask2former-swin-large-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-large-ade-semantic\n",
      "Checking model 213 keremberke/yolov8s-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-building-segmentation\n",
      "Checking model 214 nishita/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nishita/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 215 plant/segformer-b5-finetuned-segments-instryde-foot-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ plant/segformer-b5-finetuned-segments-instryde-foot-test\n",
      "Checking model 216 userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 2, 2] @ userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 217 openmmlab/upernet-swin-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ openmmlab/upernet-swin-tiny\n",
      "Checking model 218 nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n",
      "#FITNESS [1.5, 2, 2] @ nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n",
      "Checking model 219 rawjaw/metastore-segmentation\n",
      "#FITNESS ERROR: rawjaw/metastore-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/rawjaw/metastore-segmentation/main' for available files. @ rawjaw/metastore-segmentation\n",
      "Checking model 220 keremberke/yolov8s-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pcb-defect-segmentation\n",
      "Checking model 221 facebook/mask2former-swin-small-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/mask2former-swin-small-ade-semantic\n",
      "Checking model 222 microsoft/beit-large-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ microsoft/beit-large-finetuned-ade-640-640\n",
      "Checking model 223 facebook/maskformer-swin-small-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-small-ade\n",
      "Checking model 224 kasumi222/segformer-b0-finetuned-busigt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.10459743923611112, 2, 2] @ kasumi222/segformer-b0-finetuned-busigt2\n",
      "Checking model 225 facebook/maskformer-swin-base-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ facebook/maskformer-swin-base-coco\n",
      "Checking model 226 matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "#FITNESS [0.11383138020833333, 2, 2] @ matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "Checking model 227 lapix/segformer-b3-finetuned-ccagt-400-300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [0.11252604166666667, 2, 2] @ lapix/segformer-b3-finetuned-ccagt-400-300\n",
      "Checking model 228 nielsr/segformer-finetuned-sidewalk-10k-steps\n",
      "#FITNESS [1.5, 2, 2] @ nielsr/segformer-finetuned-sidewalk-10k-steps\n",
      "Checking model 229 nickmuchi/segformer-b4-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 2] @ nickmuchi/segformer-b4-finetuned-segments-sidewalk\n",
      "Checking model 230 facebook/mask2former-swin-large-ade-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 2, 1] @ facebook/mask2former-swin-large-ade-panoptic\n",
      "Checking model 0 millionhz/segformer-b0-finetuned-flame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017063033663933, 215, 2] @ millionhz/segformer-b0-finetuned-flame\n",
      "Checking model 1 bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS ERROR: bilal01/segformer-b0-finetuned-segments-sidewalk-2 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/bilal01/segformer-b0-finetuned-segments-sidewalk-2/main' for available files. @ bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 2 openmmlab/upernet-convnext-large\n",
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-convnext-large\n",
      "Checking model 3 harshm121/M3L\n",
      "#FITNESS ERROR: harshm121/M3L does not appear to have a file named config.json. Checkout 'https://huggingface.co/harshm121/M3L/main' for available files. @ harshm121/M3L\n",
      "Checking model 4 andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "#FITNESS [1.5, 215, 1] @ andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "Checking model 5 google/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [1.5, 215, 2] @ google/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 6 imageomics/BGNN-trait-segmentation\n",
      "#FITNESS ERROR: imageomics/BGNN-trait-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/imageomics/BGNN-trait-segmentation/main' for available files. @ imageomics/BGNN-trait-segmentation\n",
      "Checking model 7 hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 8 nielsr/segformer-test-v6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-test-v6\n",
      "Checking model 9 keremberke/yolov8m-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pothole-segmentation\n",
      "Checking model 10 facebook/mask2former-swin-large-mapillary-vistas-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-mapillary-vistas-semantic\n",
      "Checking model 11 shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "Checking model 12 NimaBoscarino/IS-Net_DIS-general-use\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS-general-use does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS-general-use/main' for available files. @ NimaBoscarino/IS-Net_DIS-general-use\n",
      "Checking model 13 nielsr/segformer-trainer-test-bis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-trainer-test-bis\n",
      "Checking model 14 Efferbach/segformer-finetuned-lane-10k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.001766611114016, 215, 2] @ Efferbach/segformer-finetuned-lane-10k-steps\n",
      "Checking model 15 facebook/mask2former-swin-base-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-coco-instance\n",
      "Checking model 16 shi-labs/oneformer_ade20k_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_ade20k_dinat_large\n",
      "Checking model 17 Sadiksmart0/unet\n",
      "#FITNESS ERROR: Sadiksmart0/unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sadiksmart0/unet/main' for available files. @ Sadiksmart0/unet\n",
      "Checking model 18 openmmlab/upernet-swin-base\n",
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-swin-base\n",
      "Checking model 19 koushikn/segformer-finetuned-Maize-10k-steps-sem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017417164637357, 215, 2] @ koushikn/segformer-finetuned-Maize-10k-steps-sem\n",
      "Checking model 20 facebook/mask2former-swin-large-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-large-cityscapes-panoptic\n",
      "Checking model 21 nielsr/sidewalk-semantic-demo\n",
      "#FITNESS [1.5, 215, 2] @ nielsr/sidewalk-semantic-demo\n",
      "Checking model 22 fabda/nucount\n",
      "#FITNESS ERROR: Unrecognized model in fabda/nucount. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ fabda/nucount\n",
      "Checking model 23 nvidia/segformer-b0-finetuned-cityscapes-640-1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0005131500496576, 215, 2] @ nvidia/segformer-b0-finetuned-cityscapes-640-1280\n",
      "Checking model 24 facebook/mask2former-swin-tiny-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-cityscapes-instance\n",
      "Checking model 25 nielsr/segformer-b0-finetuned-segments-sidewalk\n",
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 26 microsoft/beit-base-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ microsoft/beit-base-finetuned-ade-640-640\n",
      "Checking model 27 shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "Checking model 28 nvidia/segformer-b3-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b3-finetuned-ade-512-512\n",
      "Checking model 29 keremberke/yolov8n-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pcb-defect-segmentation\n",
      "Checking model 30 kiheh85202/yolo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at kiheh85202/yolo and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ kiheh85202/yolo\n",
      "Checking model 31 skaliy/spine-segmentation\n",
      "#FITNESS ERROR: skaliy/spine-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/spine-segmentation/main' for available files. @ skaliy/spine-segmentation\n",
      "Checking model 32 Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [1.5, 215, 2] @ Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 33 lmazzon70/deeplab-v3\n",
      "#FITNESS ERROR: lmazzon70/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/lmazzon70/deeplab-v3/main' for available files. @ lmazzon70/deeplab-v3\n",
      "Checking model 34 NimaBoscarino/IS-Net_DIS\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS/main' for available files. @ NimaBoscarino/IS-Net_DIS\n",
      "Checking model 35 keras-io/deeplabv3p-resnet50\n",
      "#FITNESS ERROR: keras-io/deeplabv3p-resnet50 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/deeplabv3p-resnet50/main' for available files. @ keras-io/deeplabv3p-resnet50\n",
      "Checking model 36 shi-labs/oneformer_ade20k_swin_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ shi-labs/oneformer_ade20k_swin_tiny\n",
      "Checking model 37 keremberke/yolov8n-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-building-segmentation\n",
      "Checking model 38 facebook/mask2former-swin-base-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-coco-panoptic\n",
      "Checking model 39 facebook/mask2former-swin-tiny-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-cityscapes-semantic\n",
      "Checking model 40 Ragweed/sidewalk-segmentation\n",
      "#FITNESS ERROR: Ragweed/sidewalk-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/Ragweed/sidewalk-segmentation/main' for available files. @ Ragweed/sidewalk-segmentation\n",
      "Checking model 41 facebook/mask2former-swin-large-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-cityscapes-instance\n",
      "Checking model 42 apple/deeplabv3-mobilevit-x-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ apple/deeplabv3-mobilevit-x-small\n",
      "Checking model 43 shi-labs/oneformer_ade20k_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ shi-labs/oneformer_ade20k_swin_large\n",
      "Checking model 44 Efferbach/mobilevit-small-10k-steps\n",
      "#FITNESS [1.5, 215, 1] @ Efferbach/mobilevit-small-10k-steps\n",
      "Checking model 45 zklee98/segformer-b1-solarModuleAnomaly-v0.1\n",
      "#FITNESS [1.5, 215, 1] @ zklee98/segformer-b1-solarModuleAnomaly-v0.1\n",
      "Checking model 46 bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "#FITNESS [1.5, 215, 1] @ bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "Checking model 47 OpenGVLab/PATH-ViTB\n",
      "#FITNESS ERROR: OpenGVLab/PATH-ViTB does not appear to have a file named config.json. Checkout 'https://huggingface.co/OpenGVLab/PATH-ViTB/main' for available files. @ OpenGVLab/PATH-ViTB\n",
      "Checking model 48 matjesg/deepflash2_demo\n",
      "#FITNESS ERROR: matjesg/deepflash2_demo does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/deepflash2_demo/main' for available files. @ matjesg/deepflash2_demo\n",
      "Checking model 49 AlmogM/segformer-b0-finetuned-fish-almogm\n",
      "#FITNESS [1.0017184520529243, 215, 2] @ AlmogM/segformer-b0-finetuned-fish-almogm\n",
      "Checking model 50 nvidia/segformer-b0-finetuned-cityscapes-512-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.000625555614055, 215, 2] @ nvidia/segformer-b0-finetuned-cityscapes-512-1024\n",
      "Checking model 51 facebook/maskformer-swin-tiny-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/maskformer-swin-tiny-ade\n",
      "Checking model 52 matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 215, 2] @ matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 53 yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "#FITNESS [1.0017899173037155, 215, 2] @ yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "Checking model 54 Onegafer/segformer-v-mesh-0\n",
      "#FITNESS [1.0015625138574156, 215, 2] @ Onegafer/segformer-v-mesh-0\n",
      "Checking model 55 keremberke/yolov8m-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-building-segmentation\n",
      "Checking model 56 edwardhuang/test-carbonate-segmentation2\n",
      "#FITNESS [1.0015531936914042, 215, 2] @ edwardhuang/test-carbonate-segmentation2\n",
      "Checking model 57 Edalik/hockey\n",
      "#FITNESS ERROR: Unrecognized model in Edalik/hockey. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Edalik/hockey\n",
      "Checking model 58 Leiyao-Cui/STRAP\n",
      "#FITNESS ERROR: Leiyao-Cui/STRAP does not appear to have a file named config.json. Checkout 'https://huggingface.co/Leiyao-Cui/STRAP/main' for available files. @ Leiyao-Cui/STRAP\n",
      "Checking model 59 openmmlab/upernet-swin-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-swin-large\n",
      "Checking model 60 facebook/mask2former-swin-tiny-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-coco-panoptic\n",
      "Checking model 61 nvidia/segformer-b4-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b4-finetuned-ade-512-512\n",
      "Checking model 62 prem-timsina/segformer-b0-finetuned-food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ prem-timsina/segformer-b0-finetuned-food\n",
      "Checking model 63 facebook/maskformer-swin-large-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/maskformer-swin-large-coco\n",
      "Checking model 64 zho/segformer-finetuned-sidewalk-10k-steps\n",
      "#FITNESS [1.5, 215, 1] @ zho/segformer-finetuned-sidewalk-10k-steps\n",
      "Checking model 65 DiTo97/binarization-segformer-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ DiTo97/binarization-segformer-b3\n",
      "Checking model 66 facebook/mask2former-swin-base-IN21k-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-base-IN21k-coco-instance\n",
      "Checking model 67 nielsr/segformer-test-v7\n",
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-test-v7\n",
      "Checking model 68 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "#FITNESS [1.0017132426127717, 215, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "Checking model 69 vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n",
      "Checking model 70 facebook/maskformer-swin-base-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/maskformer-swin-base-ade\n",
      "Checking model 71 ArturR01/segformer-b0-example-pytorch-blog\n",
      "#FITNESS [1.5, 215, 2] @ ArturR01/segformer-b0-example-pytorch-blog\n",
      "Checking model 72 mraottth/trashbot\n",
      "#FITNESS [1.5, 215, 1] @ mraottth/trashbot\n",
      "Checking model 73 jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "Checking model 74 Lewislou/cell-seg-sribd\n",
      "#FITNESS ERROR: Lewislou/cell-seg-sribd does not appear to have a file named config.json. Checkout 'https://huggingface.co/Lewislou/cell-seg-sribd/main' for available files. @ Lewislou/cell-seg-sribd\n",
      "Checking model 75 keras-io/semantic-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in keras-io/semantic-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ keras-io/semantic-segmentation\n",
      "Checking model 76 shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "#FITNESS ERROR: Unrecognized image processor in shehan97/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 77 shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "Checking model 78 yasinelh/retinal_vessel_U-Net\n",
      "#FITNESS ERROR: yasinelh/retinal_vessel_U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/yasinelh/retinal_vessel_U-Net/main' for available files. @ yasinelh/retinal_vessel_U-Net\n",
      "Checking model 79 matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "#FITNESS ERROR: matei-dorian/segformer-b0-finetuned-human-parsing does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/matei-dorian/segformer-b0-finetuned-human-parsing/main' for available files. @ matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "Checking model 80 openmmlab/upernet-convnext-small\n",
      "#FITNESS [1.001761836499523, 215, 2] @ openmmlab/upernet-convnext-small\n",
      "Checking model 81 deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "#FITNESS ERROR: deprem-ml/deprem-keras-satellite-semantic-mapping does not appear to have a file named config.json. Checkout 'https://huggingface.co/deprem-ml/deprem-keras-satellite-semantic-mapping/main' for available files. @ deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "Checking model 82 nvidia/segformer-b1-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b1-finetuned-ade-512-512\n",
      "Checking model 83 nvidia/segformer-b0-finetuned-cityscapes-768-768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b0-finetuned-cityscapes-768-768\n",
      "Checking model 84 merve/deeplab-v3\n",
      "#FITNESS ERROR: merve/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/merve/deeplab-v3/main' for available files. @ merve/deeplab-v3\n",
      "Checking model 85 facebook/mask2former-swin-small-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-small-cityscapes-semantic\n",
      "Checking model 86 facebook/mask2former-swin-large-mapillary-vistas-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-large-mapillary-vistas-panoptic\n",
      "Checking model 87 shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "Checking model 88 SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "#FITNESS ERROR: SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net/main' for available files. @ SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "Checking model 89 s3nh/SegFormer-b4-person-segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b4-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b4-person-segmentation/main' for available files. @ s3nh/SegFormer-b4-person-segmentation\n",
      "Checking model 90 imadd/segformer-b0-finetuned-segments-water-2\n",
      "#FITNESS [1.5, 215, 2] @ imadd/segformer-b0-finetuned-segments-water-2\n",
      "Checking model 91 Andreas-w/brain-classification\n",
      "#FITNESS ERROR: Andreas-w/brain-classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Andreas-w/brain-classification\n",
      "Checking model 92 nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0015143044708799, 215, 2] @ nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n",
      "Checking model 93 jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "#FITNESS [1.0017936887841958, 215, 2] @ jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "Checking model 94 s3nh/SegFormer-b0-person-segmentation\n",
      "#FITNESS ERROR: s3nh/SegFormer-b0-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b0-person-segmentation/main' for available files. @ s3nh/SegFormer-b0-person-segmentation\n",
      "Checking model 95 keremberke/yolov8s-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pothole-segmentation\n",
      "Checking model 96 hf-tiny-model-private/tiny-random-DetrForSegmentation\n",
      "#FITNESS ERROR: list index out of range @ hf-tiny-model-private/tiny-random-DetrForSegmentation\n",
      "Checking model 97 openmmlab/upernet-convnext-xlarge\n",
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-convnext-xlarge\n",
      "Checking model 98 andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0016100137253632, 215, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n",
      "Checking model 99 jtsang4/test-model\n",
      "#FITNESS ERROR: jtsang4/test-model does not appear to have a file named config.json. Checkout 'https://huggingface.co/jtsang4/test-model/main' for available files. @ jtsang4/test-model\n",
      "Checking model 100 facebook/mask2former-swin-base-IN21k-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-base-IN21k-ade-semantic\n",
      "Checking model 101 turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 215, 1] @ turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 102 nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0010228636861853, 215, 2] @ nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n",
      "Checking model 103 leonelhs/faceparser\n",
      "#FITNESS ERROR: Unrecognized model in leonelhs/faceparser. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ leonelhs/faceparser\n",
      "Checking model 104 mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 215, 2] @ mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 105 magicmirror/segformer-b4-finetuned-segments-torso\n",
      "#FITNESS ERROR: magicmirror/segformer-b4-finetuned-segments-torso is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ magicmirror/segformer-b4-finetuned-segments-torso\n",
      "Checking model 106 nvidia/segformer-b2-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b2-finetuned-ade-512-512\n",
      "Checking model 107 Saurabh1105/MMDet\n",
      "#FITNESS ERROR: Saurabh1105/MMDet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Saurabh1105/MMDet/main' for available files. @ Saurabh1105/MMDet\n",
      "Checking model 108 optimum/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model optimum/segformer-b0-finetuned-ade-512-512 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ optimum/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 109 shi-labs/oneformer_coco_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ shi-labs/oneformer_coco_swin_large\n",
      "Checking model 110 facebook/maskformer-swin-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/maskformer-swin-large-ade\n",
      "Checking model 111 apple/deeplabv3-mobilevit-small\n",
      "#FITNESS [1.5, 215, 1] @ apple/deeplabv3-mobilevit-small\n",
      "Checking model 112 Plsek/CADET-v1\n",
      "#FITNESS ERROR: Plsek/CADET-v1 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Plsek/CADET-v1/main' for available files. @ Plsek/CADET-v1\n",
      "Checking model 113 Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: Robindboer/segformer-b5-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "Checking model 114 facebook/mask2former-swin-tiny-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-ade-semantic\n",
      "Checking model 115 shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "Checking model 116 facebook/mask2former-swin-large-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-coco-panoptic\n",
      "Checking model 117 CIDAS/clipseg-rd16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd16\n",
      "Checking model 118 segments-tobias/segformer-b3-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0015231522041579, 215, 2] @ segments-tobias/segformer-b3-finetuned-segments-sidewalk\n",
      "Checking model 119 apple/mobilevitv2-1.0-voc-deeplabv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Unrecognized image processor in apple/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ apple/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 120 nielsr/segformer-test-sidewalk-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-test-sidewalk-v2\n",
      "Checking model 121 facebook/maskformer-swin-small-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/maskformer-swin-small-coco\n",
      "Checking model 122 Intel/dpt-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ Intel/dpt-large-ade\n",
      "Checking model 123 mattmdjaga/segformer_b2_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0010350792200151, 215, 2] @ mattmdjaga/segformer_b2_clothes\n",
      "Checking model 124 malra/segformer-b5-segments-warehouse1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ malra/segformer-b5-segments-warehouse1\n",
      "Checking model 125 Matthijs/deeplabv3-mobilevit-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Matthijs/deeplabv3-mobilevit-small were not used when initializing MobileViTForSemanticSegmentation: ['mobilevit.encoder.layer.4.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.weight', 'seg_head.classifier.classifier.conv.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.layernorm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.conv.weight', 'seg_head.aspp.convs.2.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.num_batches_tracked', 'seg_head.aspp.project.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.weight', 'seg_head.aspp.convs.0.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.weight', 'seg_head.aspp.convs.1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_mean', 'seg_head.aspp.convs.0.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.2.norm.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.bias', 'seg_head.aspp.convs.3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.weight', 'seg_head.aspp.convs.3.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.layernorm.weight', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.bias', 'seg_head.aspp.project.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.bias', 'mobilevit.conv_stem.norm.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.4.1.fusion.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.weight', 'mobilevit.conv_stem.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_mean', 'seg_head.aspp.convs.2.norm.running_mean', 'seg_head.aspp.project.norm.bias', 'seg_head.aspp.project.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.conv.weight', 'seg_head.aspp.convs.0.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.num_batches_tracked', 'seg_head.aspp.convs.1.norm.running_mean', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.bias', 'seg_head.aspp.convs.3.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.fusion.norm.bias', 'mobilevit.conv_stem.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.bias', 'seg_head.aspp.convs.3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.conv.weight', 'seg_head.aspp.convs.0.norm.running_var', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.weight', 'seg_head.aspp.convs.3.norm.running_var', 'mobilevit.encoder.layer.3.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.weight', 'seg_head.aspp.project.norm.weight', 'mobilevit.encoder.layer.4.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_mean', 'seg_head.aspp.project.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.weight', 'seg_head.aspp.convs.2.norm.bias', 'seg_head.aspp.convs.1.norm.running_var', 'mobilevit.encoder.layer.2.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.bias', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.weight', 'seg_head.aspp.convs.0.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_var', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.1.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.4.0.reduce_1x1.conv.weight', 'seg_head.aspp.convs.1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_var', 'mobilevit.conv_stem.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.num_batches_tracked', 'seg_head.classifier.classifier.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_mean', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_var', 'mobilevit.conv_stem.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_var', 'seg_head.aspp.convs.2.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.weight', 'seg_head.aspp.convs.1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.1.0.conv_3x3.conv.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.fusion.conv.weight', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.bias', 'seg_head.aspp.convs.0.conv.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.weight', 'seg_head.aspp.convs.2.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.weight', 'mobilevit.conv_stem.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.weight']\n",
      "- This IS expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForSemanticSegmentation were not initialized from the model checkpoint at Matthijs/deeplabv3-mobilevit-small and are newly initialized: ['mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.convs.3.normalization.bias', 'mobilevit.encoder.layer.3.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.weight', 'segmentation_head.aspp.convs.2.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.2.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.2.fusion.normalization.weight', 'segmentation_head.aspp.convs.2.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.4.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_projection.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.fusion.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.convolution.weight', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.4.fusion.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.weight', 'mobilevit.encoder.layer.4.fusion.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.aspp.project.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.weight', 'mobilevit.conv_stem.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.fusion.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.4.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.3.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.weight', 'mobilevit.conv_stem.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.bias', 'segmentation_head.aspp.convs.1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.weight', 'segmentation_head.aspp.convs.1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.weight', 'segmentation_head.aspp.convs.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_var', 'segmentation_head.classifier.convolution.bias', 'segmentation_head.aspp.convs.0.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.bias', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.convolution.weight', 'mobilevit.conv_stem.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.conv_stem.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.bias', 'segmentation_head.aspp.convs.0.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.layernorm.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_mean', 'segmentation_head.aspp.convs.1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.weight', 'mobilevit.conv_stem.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.fusion.normalization.running_mean', 'segmentation_head.aspp.project.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.3.fusion.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.classifier.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.project.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_var', 'segmentation_head.aspp.convs.2.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.weight', 'segmentation_head.aspp.convs.0.convolution.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_var', 'segmentation_head.aspp.convs.1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.0.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.4.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.4.conv_projection.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.2.conv_projection.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_kxk.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.1.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.weight', 'segmentation_head.aspp.convs.0.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.weight', 'segmentation_head.aspp.convs.0.normalization.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.bias', 'segmentation_head.aspp.convs.2.normalization.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.3.fusion.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.layernorm.bias', 'segmentation_head.aspp.convs.3.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.bias', 'mobilevit.conv_stem.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.convolution.weight', 'segmentation_head.aspp.project.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.bias', 'segmentation_head.aspp.convs.2.normalization.bias', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.4.conv_projection.normalization.weight', 'mobilevit.encoder.layer.2.fusion.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_kxk.convolution.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ Matthijs/deeplabv3-mobilevit-small\n",
      "Checking model 126 zoheb/mit-b5-finetuned-sidewalk-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ zoheb/mit-b5-finetuned-sidewalk-semantic\n",
      "Checking model 127 shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "Checking model 128 facebook/mask2former-swin-small-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-small-coco-instance\n",
      "Checking model 129 shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 215, 2] @ shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 130 skaliy/endometrical_cancer_segmentation\n",
      "#FITNESS ERROR: skaliy/endometrical_cancer_segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/endometrical_cancer_segmentation/main' for available files. @ skaliy/endometrical_cancer_segmentation\n",
      "Checking model 131 Azarthehulk/Image_preprocessing_basics\n",
      "#FITNESS ERROR: Azarthehulk/Image_preprocessing_basics does not appear to have a file named config.json. Checkout 'https://huggingface.co/Azarthehulk/Image_preprocessing_basics/main' for available files. @ Azarthehulk/Image_preprocessing_basics\n",
      "Checking model 132 facebook/mask2former-swin-small-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-small-coco-panoptic\n",
      "Checking model 133 facebook/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/detr-resnet-50-panoptic\n",
      "Checking model 134 shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "Checking model 135 s3nh/SegFormer-b5-person-segm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b5-person-segm does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b5-person-segm/main' for available files. @ s3nh/SegFormer-b5-person-segm\n",
      "Checking model 136 facebook/mask2former-swin-large-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-large-cityscapes-semantic\n",
      "Checking model 137 Robindboer/imageseg\n",
      "#FITNESS ERROR: Robindboer/imageseg does not appear to have a file named config.json. Checkout 'https://huggingface.co/Robindboer/imageseg/main' for available files. @ Robindboer/imageseg\n",
      "Checking model 138 facebook/mask2former-swin-base-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-ade-semantic\n",
      "Checking model 139 sayakpaul/mit-b0-finetuned-sidewalk-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-sidewalk-semantic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-sidewalk-semantic\n",
      "Checking model 140 chainyo/segformer-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017397775259158, 215, 2] @ chainyo/segformer-sidewalk\n",
      "Checking model 141 facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic were not used when initializing Mask2FormerForUniversalSegmentation: ['model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.model.layernorm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.0.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.3.bias', 'model.pixel_level_module.encoder.hidden_states_norms.3.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.embeddings.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.0.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.model.layernorm.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.model.embeddings.norm.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.bias']\n",
      "- This IS expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic and are newly initialized: ['model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n",
      "Checking model 142 thiagohersan/maskformer-satellite-trees\n",
      "#FITNESS ERROR: thiagohersan/maskformer-satellite-trees is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ thiagohersan/maskformer-satellite-trees\n",
      "Checking model 143 shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "Checking model 144 Efferbach/segformer-finetuned-lane-1k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ Efferbach/segformer-finetuned-lane-1k-steps\n",
      "Checking model 145 irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n",
      "Checking model 146 chainyo/segformer-b1-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ chainyo/segformer-b1-sidewalk\n",
      "Checking model 147 facebook/mask2former-swin-base-IN21k-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-IN21k-cityscapes-instance\n",
      "Checking model 148 facebook/mask2former-swin-large-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-large-coco-instance\n",
      "Checking model 149 facebook/detr-resnet-101-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ facebook/detr-resnet-101-panoptic\n",
      "Checking model 150 nielsr/segformer-test-v5\n",
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-test-v5\n",
      "Checking model 151 openmmlab/upernet-convnext-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0006861150548583, 215, 2] @ openmmlab/upernet-convnext-tiny\n",
      "Checking model 152 nielsr/segformer-trainer-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-trainer-test\n",
      "Checking model 153 sayakpaul/mit-b0-finetuned-pets\n",
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-pets with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-pets\n",
      "Checking model 154 thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017050136792633, 215, 2] @ thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 155 cvappbakkers/techniparts\n",
      "#FITNESS ERROR: cvappbakkers/techniparts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ cvappbakkers/techniparts\n",
      "Checking model 156 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "#FITNESS [1.5, 215, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "Checking model 157 Abhilashvj/clipseg-rd64-refined-copy\n",
      "#FITNESS ERROR: Abhilashvj/clipseg-rd64-refined-copy does not appear to have a file named config.json. Checkout 'https://huggingface.co/Abhilashvj/clipseg-rd64-refined-copy/main' for available files. @ Abhilashvj/clipseg-rd64-refined-copy\n",
      "Checking model 158 Xpitfire/segformer-finetuned-segments-cmp-facade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ Xpitfire/segformer-finetuned-segments-cmp-facade\n",
      "Checking model 159 nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n",
      "Checking model 160 CIDAS/clipseg-rd64-refined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64-refined\n",
      "Checking model 161 alanoix/segformer_b0_flair_one\n",
      "#FITNESS ERROR: alanoix/segformer_b0_flair_one does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/alanoix/segformer_b0_flair_one/main' for available files. @ alanoix/segformer_b0_flair_one\n",
      "Checking model 162 bilal01/segformer-b0-finetuned-segments-test\n",
      "#FITNESS [1.5, 215, 2] @ bilal01/segformer-b0-finetuned-segments-test\n",
      "Checking model 163 Xenova/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model Xenova/detr-resnet-50-panoptic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.detr.modeling_detr.DetrForSegmentation'>). @ Xenova/detr-resnet-50-panoptic\n",
      "Checking model 164 keremberke/yolov8m-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pcb-defect-segmentation\n",
      "Checking model 165 Efferbach/mobilenet_v2_1-10k-steps\n",
      "#FITNESS [1.5, 215, 1] @ Efferbach/mobilenet_v2_1-10k-steps\n",
      "Checking model 166 jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b3-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "Checking model 167 matjesg/cFOS_in_HC\n",
      "#FITNESS ERROR: matjesg/cFOS_in_HC does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/cFOS_in_HC/main' for available files. @ matjesg/cFOS_in_HC\n",
      "Checking model 168 fcakyon/test-model\n",
      "#FITNESS ERROR: 'v8' @ fcakyon/test-model\n",
      "Checking model 169 androks/rembg\n",
      "#FITNESS ERROR: androks/rembg does not appear to have a file named config.json. Checkout 'https://huggingface.co/androks/rembg/main' for available files. @ androks/rembg\n",
      "Checking model 170 facebook/mask2former-swin-tiny-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-cityscapes-panoptic\n",
      "Checking model 171 openmmlab/upernet-convnext-base\n",
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-convnext-base\n",
      "Checking model 172 segments-tobias/segformer-b0-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017050136792633, 215, 2] @ segments-tobias/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 173 q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "#FITNESS [1.5, 215, 1] @ q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "Checking model 174 nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n",
      "Checking model 175 jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "Checking model 176 Narsil/pet-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in Narsil/pet-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Narsil/pet-segmentation\n",
      "Checking model 177 jakka/segformer-b0-finetuned-warehouse-part-1-V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ jakka/segformer-b0-finetuned-warehouse-part-1-V2\n",
      "Checking model 178 facebook/maskformer-swin-tiny-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/maskformer-swin-tiny-coco\n",
      "Checking model 179 Mendel192/san\n",
      "#FITNESS ERROR: Mendel192/san does not appear to have a file named config.json. Checkout 'https://huggingface.co/Mendel192/san/main' for available files. @ Mendel192/san\n",
      "Checking model 180 facebook/mask2former-swin-small-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-small-cityscapes-panoptic\n",
      "Checking model 181 facebook/mask2former-swin-tiny-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-tiny-coco-instance\n",
      "Checking model 182 nvidia/segformer-b5-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b5-finetuned-ade-640-640\n",
      "Checking model 183 openmmlab/upernet-swin-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-swin-small\n",
      "Checking model 184 shi-labs/oneformer_cityscapes_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_cityscapes_dinat_large\n",
      "Checking model 185 keras-io/monocular-depth-estimation\n",
      "#FITNESS ERROR: keras-io/monocular-depth-estimation does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/monocular-depth-estimation/main' for available files. @ keras-io/monocular-depth-estimation\n",
      "Checking model 186 mraottth/trashbot_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ mraottth/trashbot_v1\n",
      "Checking model 187 yahaoh/ddh-maskrcnn\n",
      "#FITNESS ERROR: yahaoh/ddh-maskrcnn does not appear to have a file named config.json. Checkout 'https://huggingface.co/yahaoh/ddh-maskrcnn/main' for available files. @ yahaoh/ddh-maskrcnn\n",
      "Checking model 188 shi-labs/oneformer_coco_dinat_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_coco_dinat_large\n",
      "Checking model 189 nielsr/segformer-finetuned-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nielsr/segformer-finetuned-sidewalk\n",
      "Checking model 190 shi-labs/oneformer_cityscapes_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ shi-labs/oneformer_cityscapes_swin_large\n",
      "Checking model 191 reannayang/segformer-b0-pavement\n",
      "#FITNESS [1.00175439742296, 215, 2] @ reannayang/segformer-b0-pavement\n",
      "Checking model 192 maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "#FITNESS ERROR: maryann-gitonga/brain-tumor-segmentation-3d-attention-unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/maryann-gitonga/brain-tumor-segmentation-3d-attention-unet/main' for available files. @ maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "Checking model 193 nvidia/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 194 jonathandinu/face-parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017611642218396, 215, 2] @ jonathandinu/face-parsing\n",
      "Checking model 195 keremberke/yolov8n-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pothole-segmentation\n",
      "Checking model 196 ksahn/pid-s\n",
      "#FITNESS ERROR: ksahn/pid-s is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ ksahn/pid-s\n",
      "Checking model 197 apple/deeplabv3-mobilevit-xx-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ apple/deeplabv3-mobilevit-xx-small\n",
      "Checking model 198 nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n",
      "Checking model 199 facebook/mask2former-swin-small-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-small-cityscapes-instance\n",
      "Checking model 200 facebook/detr-resnet-50-dc5-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/detr-resnet-50-dc5-panoptic\n",
      "Checking model 201 keja/deeplab-v3\n",
      "#FITNESS ERROR: keja/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keja/deeplab-v3/main' for available files. @ keja/deeplab-v3\n",
      "Checking model 202 Sevenlee/kkk\n",
      "#FITNESS ERROR: Sevenlee/kkk does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sevenlee/kkk/main' for available files. @ Sevenlee/kkk\n",
      "Checking model 203 jakka/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ jakka/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 204 Niceforo/teste\n",
      "#FITNESS ERROR: Niceforo/teste does not appear to have a file named config.json. Checkout 'https://huggingface.co/Niceforo/teste/main' for available files. @ Niceforo/teste\n",
      "Checking model 205 mattmdjaga/segformer_b0_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0004596705887467, 215, 2] @ mattmdjaga/segformer_b0_clothes\n",
      "Checking model 206 cosmobaby/ka\n",
      "#FITNESS ERROR: cosmobaby/ka does not appear to have a file named config.json. Checkout 'https://huggingface.co/cosmobaby/ka/main' for available files. @ cosmobaby/ka\n",
      "Checking model 207 CIDAS/clipseg-rd64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64\n",
      "Checking model 208 voitl/unet_plus_plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: No module named 'segmentation_models_pytorch' @ voitl/unet_plus_plus\n",
      "Checking model 209 facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n",
      "Checking model 210 Lewislou/cellseg_sribd\n",
      "#FITNESS ERROR: 'cell_sribd' @ Lewislou/cellseg_sribd\n",
      "Checking model 211 malra/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ malra/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 212 facebook/mask2former-swin-large-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0001745059481897, 215, 2] @ facebook/mask2former-swin-large-ade-semantic\n",
      "Checking model 213 keremberke/yolov8s-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-building-segmentation\n",
      "Checking model 214 nishita/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nishita/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 215 plant/segformer-b5-finetuned-segments-instryde-foot-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.001227902605809, 215, 2] @ plant/segformer-b5-finetuned-segments-instryde-foot-test\n",
      "Checking model 216 userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 215, 1] @ userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 217 openmmlab/upernet-swin-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ openmmlab/upernet-swin-tiny\n",
      "Checking model 218 nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n",
      "#FITNESS [1.5, 215, 2] @ nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n",
      "Checking model 219 rawjaw/metastore-segmentation\n",
      "#FITNESS ERROR: rawjaw/metastore-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/rawjaw/metastore-segmentation/main' for available files. @ rawjaw/metastore-segmentation\n",
      "Checking model 220 keremberke/yolov8s-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pcb-defect-segmentation\n",
      "Checking model 221 facebook/mask2former-swin-small-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-small-ade-semantic\n",
      "Checking model 222 microsoft/beit-large-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ microsoft/beit-large-finetuned-ade-640-640\n",
      "Checking model 223 facebook/maskformer-swin-small-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/maskformer-swin-small-ade\n",
      "Checking model 224 kasumi222/segformer-b0-finetuned-busigt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017830329351463, 215, 2] @ kasumi222/segformer-b0-finetuned-busigt2\n",
      "Checking model 225 facebook/maskformer-swin-base-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 1] @ facebook/maskformer-swin-base-coco\n",
      "Checking model 226 matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "#FITNESS [1.0017710112218943, 215, 2] @ matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "Checking model 227 lapix/segformer-b3-finetuned-ccagt-400-300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017899013212572, 215, 2] @ lapix/segformer-b3-finetuned-ccagt-400-300\n",
      "Checking model 228 nielsr/segformer-finetuned-sidewalk-10k-steps\n",
      "#FITNESS [1.5, 215, 1] @ nielsr/segformer-finetuned-sidewalk-10k-steps\n",
      "Checking model 229 nickmuchi/segformer-b4-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 215, 2] @ nickmuchi/segformer-b4-finetuned-segments-sidewalk\n",
      "Checking model 230 facebook/mask2former-swin-large-ade-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-ade-panoptic\n",
      "Checking model 0 millionhz/segformer-b0-finetuned-flame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ millionhz/segformer-b0-finetuned-flame\n",
      "Checking model 1 bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS ERROR: bilal01/segformer-b0-finetuned-segments-sidewalk-2 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/bilal01/segformer-b0-finetuned-segments-sidewalk-2/main' for available files. @ bilal01/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 2 openmmlab/upernet-convnext-large\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-convnext-large\n",
      "Checking model 3 harshm121/M3L\n",
      "#FITNESS ERROR: harshm121/M3L does not appear to have a file named config.json. Checkout 'https://huggingface.co/harshm121/M3L/main' for available files. @ harshm121/M3L\n",
      "Checking model 4 andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "#FITNESS [1.0031810804763266, 256, 2] @ andrewljohnson/segformer-b0-finetuned-magic-cards-230117\n",
      "Checking model 5 google/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [1.0025959033355685, 256, 2] @ google/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 6 imageomics/BGNN-trait-segmentation\n",
      "#FITNESS ERROR: imageomics/BGNN-trait-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/imageomics/BGNN-trait-segmentation/main' for available files. @ imageomics/BGNN-trait-segmentation\n",
      "Checking model 7 hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ hufanyoung/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 8 nielsr/segformer-test-v6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-test-v6\n",
      "Checking model 9 keremberke/yolov8m-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pothole-segmentation\n",
      "Checking model 10 facebook/mask2former-swin-large-mapillary-vistas-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0009265801114506, 256, 2] @ facebook/mask2former-swin-large-mapillary-vistas-semantic\n",
      "Checking model 11 shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2021-instance\n",
      "Checking model 12 NimaBoscarino/IS-Net_DIS-general-use\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS-general-use does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS-general-use/main' for available files. @ NimaBoscarino/IS-Net_DIS-general-use\n",
      "Checking model 13 nielsr/segformer-trainer-test-bis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-trainer-test-bis\n",
      "Checking model 14 Efferbach/segformer-finetuned-lane-10k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ Efferbach/segformer-finetuned-lane-10k-steps\n",
      "Checking model 15 facebook/mask2former-swin-base-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-coco-instance\n",
      "Checking model 16 shi-labs/oneformer_ade20k_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_ade20k_dinat_large\n",
      "Checking model 17 Sadiksmart0/unet\n",
      "#FITNESS ERROR: Sadiksmart0/unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sadiksmart0/unet/main' for available files. @ Sadiksmart0/unet\n",
      "Checking model 18 openmmlab/upernet-swin-base\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-swin-base\n",
      "Checking model 19 koushikn/segformer-finetuned-Maize-10k-steps-sem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0006543861574995, 256, 2] @ koushikn/segformer-finetuned-Maize-10k-steps-sem\n",
      "Checking model 20 facebook/mask2former-swin-large-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.002006683086257, 256, 2] @ facebook/mask2former-swin-large-cityscapes-panoptic\n",
      "Checking model 21 nielsr/sidewalk-semantic-demo\n",
      "#FITNESS [1.5, 256, 2] @ nielsr/sidewalk-semantic-demo\n",
      "Checking model 22 fabda/nucount\n",
      "#FITNESS ERROR: Unrecognized model in fabda/nucount. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ fabda/nucount\n",
      "Checking model 23 nvidia/segformer-b0-finetuned-cityscapes-640-1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b0-finetuned-cityscapes-640-1280\n",
      "Checking model 24 facebook/mask2former-swin-tiny-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-tiny-cityscapes-instance\n",
      "Checking model 25 nielsr/segformer-b0-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 26 microsoft/beit-base-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ microsoft/beit-base-finetuned-ade-640-640\n",
      "Checking model 27 shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2021-instance\n",
      "Checking model 28 nvidia/segformer-b3-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b3-finetuned-ade-512-512\n",
      "Checking model 29 keremberke/yolov8n-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pcb-defect-segmentation\n",
      "Checking model 30 kiheh85202/yolo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at kiheh85202/yolo and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ kiheh85202/yolo\n",
      "Checking model 31 skaliy/spine-segmentation\n",
      "#FITNESS ERROR: skaliy/spine-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/spine-segmentation/main' for available files. @ skaliy/spine-segmentation\n",
      "Checking model 32 Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "#FITNESS [1.0025959033355685, 256, 2] @ Matthijs/deeplabv3_mobilenet_v2_1.0_513\n",
      "Checking model 33 lmazzon70/deeplab-v3\n",
      "#FITNESS ERROR: lmazzon70/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/lmazzon70/deeplab-v3/main' for available files. @ lmazzon70/deeplab-v3\n",
      "Checking model 34 NimaBoscarino/IS-Net_DIS\n",
      "#FITNESS ERROR: NimaBoscarino/IS-Net_DIS does not appear to have a file named config.json. Checkout 'https://huggingface.co/NimaBoscarino/IS-Net_DIS/main' for available files. @ NimaBoscarino/IS-Net_DIS\n",
      "Checking model 35 keras-io/deeplabv3p-resnet50\n",
      "#FITNESS ERROR: keras-io/deeplabv3p-resnet50 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/deeplabv3p-resnet50/main' for available files. @ keras-io/deeplabv3p-resnet50\n",
      "Checking model 36 shi-labs/oneformer_ade20k_swin_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ shi-labs/oneformer_ade20k_swin_tiny\n",
      "Checking model 37 keremberke/yolov8n-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-building-segmentation\n",
      "Checking model 38 facebook/mask2former-swin-base-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0030717016649038, 256, 2] @ facebook/mask2former-swin-base-coco-panoptic\n",
      "Checking model 39 facebook/mask2former-swin-tiny-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0009684656742153, 256, 2] @ facebook/mask2former-swin-tiny-cityscapes-semantic\n",
      "Checking model 40 Ragweed/sidewalk-segmentation\n",
      "#FITNESS ERROR: Ragweed/sidewalk-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/Ragweed/sidewalk-segmentation/main' for available files. @ Ragweed/sidewalk-segmentation\n",
      "Checking model 41 facebook/mask2former-swin-large-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-large-cityscapes-instance\n",
      "Checking model 42 apple/deeplabv3-mobilevit-x-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0029754040460601, 256, 2] @ apple/deeplabv3-mobilevit-x-small\n",
      "Checking model 43 shi-labs/oneformer_ade20k_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0004208733153281, 256, 2] @ shi-labs/oneformer_ade20k_swin_large\n",
      "Checking model 44 Efferbach/mobilevit-small-10k-steps\n",
      "#FITNESS [1.5, 256, 1] @ Efferbach/mobilevit-small-10k-steps\n",
      "Checking model 45 zklee98/segformer-b1-solarModuleAnomaly-v0.1\n",
      "#FITNESS [1.5, 256, 2] @ zklee98/segformer-b1-solarModuleAnomaly-v0.1\n",
      "Checking model 46 bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "#FITNESS [1.5, 256, 1] @ bilal01/segformer-b0-finetuned-segments-stamp-verification\n",
      "Checking model 47 OpenGVLab/PATH-ViTB\n",
      "#FITNESS ERROR: OpenGVLab/PATH-ViTB does not appear to have a file named config.json. Checkout 'https://huggingface.co/OpenGVLab/PATH-ViTB/main' for available files. @ OpenGVLab/PATH-ViTB\n",
      "Checking model 48 matjesg/deepflash2_demo\n",
      "#FITNESS ERROR: matjesg/deepflash2_demo does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/deepflash2_demo/main' for available files. @ matjesg/deepflash2_demo\n",
      "Checking model 49 AlmogM/segformer-b0-finetuned-fish-almogm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.003005294079197, 256, 2] @ AlmogM/segformer-b0-finetuned-fish-almogm\n",
      "Checking model 50 nvidia/segformer-b0-finetuned-cityscapes-512-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b0-finetuned-cityscapes-512-1024\n",
      "Checking model 51 facebook/maskformer-swin-tiny-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0007629259978192, 256, 2] @ facebook/maskformer-swin-tiny-ade\n",
      "Checking model 52 matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 256, 2] @ matnun/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 53 yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "#FITNESS [1.5, 256, 2] @ yiming19/segformer-b0-finetuned-segments-construction-1\n",
      "Checking model 54 Onegafer/segformer-v-mesh-0\n",
      "#FITNESS [1.0032539050315061, 256, 2] @ Onegafer/segformer-v-mesh-0\n",
      "Checking model 55 keremberke/yolov8m-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-building-segmentation\n",
      "Checking model 56 edwardhuang/test-carbonate-segmentation2\n",
      "#FITNESS [1.5, 256, 2] @ edwardhuang/test-carbonate-segmentation2\n",
      "Checking model 57 Edalik/hockey\n",
      "#FITNESS ERROR: Unrecognized model in Edalik/hockey. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Edalik/hockey\n",
      "Checking model 58 Leiyao-Cui/STRAP\n",
      "#FITNESS ERROR: Leiyao-Cui/STRAP does not appear to have a file named config.json. Checkout 'https://huggingface.co/Leiyao-Cui/STRAP/main' for available files. @ Leiyao-Cui/STRAP\n",
      "Checking model 59 openmmlab/upernet-swin-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-swin-large\n",
      "Checking model 60 facebook/mask2former-swin-tiny-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.002036207785764, 256, 2] @ facebook/mask2former-swin-tiny-coco-panoptic\n",
      "Checking model 61 nvidia/segformer-b4-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0019489530140628, 256, 2] @ nvidia/segformer-b4-finetuned-ade-512-512\n",
      "Checking model 62 prem-timsina/segformer-b0-finetuned-food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0031043786119322, 256, 2] @ prem-timsina/segformer-b0-finetuned-food\n",
      "Checking model 63 facebook/maskformer-swin-large-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-large-coco\n",
      "Checking model 64 zho/segformer-finetuned-sidewalk-10k-steps\n",
      "#FITNESS [1.5, 256, 2] @ zho/segformer-finetuned-sidewalk-10k-steps\n",
      "Checking model 65 DiTo97/binarization-segformer-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0027540775049664, 256, 2] @ DiTo97/binarization-segformer-b3\n",
      "Checking model 66 facebook/mask2former-swin-base-IN21k-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-base-IN21k-coco-instance\n",
      "Checking model 67 nielsr/segformer-test-v7\n",
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-test-v7\n",
      "Checking model 68 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "#FITNESS [1.0016856998570287, 256, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-3\n",
      "Checking model 69 vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ vhug/segformer-b0-finetuned-xorder-dish-segmentation-trial\n",
      "Checking model 70 facebook/maskformer-swin-base-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.000425796220109, 256, 2] @ facebook/maskformer-swin-base-ade\n",
      "Checking model 71 ArturR01/segformer-b0-example-pytorch-blog\n",
      "#FITNESS [1.5, 256, 2] @ ArturR01/segformer-b0-example-pytorch-blog\n",
      "Checking model 72 mraottth/trashbot\n",
      "#FITNESS [1.5, 256, 1] @ mraottth/trashbot\n",
      "Checking model 73 jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds-2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds-2\n",
      "Checking model 74 Lewislou/cell-seg-sribd\n",
      "#FITNESS ERROR: Lewislou/cell-seg-sribd does not appear to have a file named config.json. Checkout 'https://huggingface.co/Lewislou/cell-seg-sribd/main' for available files. @ Lewislou/cell-seg-sribd\n",
      "Checking model 75 keras-io/semantic-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in keras-io/semantic-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ keras-io/semantic-segmentation\n",
      "Checking model 76 shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "#FITNESS ERROR: Unrecognized image processor in shehan97/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ shehan97/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 77 shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-base-IN21k-youtubevis-2019-instance\n",
      "Checking model 78 yasinelh/retinal_vessel_U-Net\n",
      "#FITNESS ERROR: yasinelh/retinal_vessel_U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/yasinelh/retinal_vessel_U-Net/main' for available files. @ yasinelh/retinal_vessel_U-Net\n",
      "Checking model 79 matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "#FITNESS ERROR: matei-dorian/segformer-b0-finetuned-human-parsing does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/matei-dorian/segformer-b0-finetuned-human-parsing/main' for available files. @ matei-dorian/segformer-b0-finetuned-human-parsing\n",
      "Checking model 80 openmmlab/upernet-convnext-small\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-convnext-small\n",
      "Checking model 81 deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "#FITNESS ERROR: deprem-ml/deprem-keras-satellite-semantic-mapping does not appear to have a file named config.json. Checkout 'https://huggingface.co/deprem-ml/deprem-keras-satellite-semantic-mapping/main' for available files. @ deprem-ml/deprem-keras-satellite-semantic-mapping\n",
      "Checking model 82 nvidia/segformer-b1-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b1-finetuned-ade-512-512\n",
      "Checking model 83 nvidia/segformer-b0-finetuned-cityscapes-768-768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b0-finetuned-cityscapes-768-768\n",
      "Checking model 84 merve/deeplab-v3\n",
      "#FITNESS ERROR: merve/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/merve/deeplab-v3/main' for available files. @ merve/deeplab-v3\n",
      "Checking model 85 facebook/mask2former-swin-small-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0010140334143678, 256, 2] @ facebook/mask2former-swin-small-cityscapes-semantic\n",
      "Checking model 86 facebook/mask2former-swin-large-mapillary-vistas-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-large-mapillary-vistas-panoptic\n",
      "Checking model 87 shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2019-instance\n",
      "Checking model 88 SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "#FITNESS ERROR: SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net does not appear to have a file named config.json. Checkout 'https://huggingface.co/SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net/main' for available files. @ SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net\n",
      "Checking model 89 s3nh/SegFormer-b4-person-segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b4-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b4-person-segmentation/main' for available files. @ s3nh/SegFormer-b4-person-segmentation\n",
      "Checking model 90 imadd/segformer-b0-finetuned-segments-water-2\n",
      "#FITNESS [1.5, 256, 2] @ imadd/segformer-b0-finetuned-segments-water-2\n",
      "Checking model 91 Andreas-w/brain-classification\n",
      "#FITNESS ERROR: Andreas-w/brain-classification is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Andreas-w/brain-classification\n",
      "Checking model 92 nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n",
      "Checking model 93 jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "#FITNESS [1.5, 256, 2] @ jordibeen/segformer-b0-finetuned-segments-mopeds-3\n",
      "Checking model 94 s3nh/SegFormer-b0-person-segmentation\n",
      "#FITNESS ERROR: s3nh/SegFormer-b0-person-segmentation does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b0-person-segmentation/main' for available files. @ s3nh/SegFormer-b0-person-segmentation\n",
      "Checking model 95 keremberke/yolov8s-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pothole-segmentation\n",
      "Checking model 96 hf-tiny-model-private/tiny-random-DetrForSegmentation\n",
      "#FITNESS ERROR: list index out of range @ hf-tiny-model-private/tiny-random-DetrForSegmentation\n",
      "Checking model 97 openmmlab/upernet-convnext-xlarge\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-convnext-xlarge\n",
      "Checking model 98 andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.002306337439415, 256, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117\n",
      "Checking model 99 jtsang4/test-model\n",
      "#FITNESS ERROR: jtsang4/test-model does not appear to have a file named config.json. Checkout 'https://huggingface.co/jtsang4/test-model/main' for available files. @ jtsang4/test-model\n",
      "Checking model 100 facebook/mask2former-swin-base-IN21k-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-base-IN21k-ade-semantic\n",
      "Checking model 101 turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 256, 1] @ turcuciprian/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 102 nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b1-finetuned-cityscapes-1024-1024\n",
      "Checking model 103 leonelhs/faceparser\n",
      "#FITNESS ERROR: Unrecognized model in leonelhs/faceparser. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ leonelhs/faceparser\n",
      "Checking model 104 mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.5, 256, 2] @ mnosouhi96/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 105 magicmirror/segformer-b4-finetuned-segments-torso\n",
      "#FITNESS ERROR: magicmirror/segformer-b4-finetuned-segments-torso is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ magicmirror/segformer-b4-finetuned-segments-torso\n",
      "Checking model 106 nvidia/segformer-b2-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b2-finetuned-ade-512-512\n",
      "Checking model 107 Saurabh1105/MMDet\n",
      "#FITNESS ERROR: Saurabh1105/MMDet does not appear to have a file named config.json. Checkout 'https://huggingface.co/Saurabh1105/MMDet/main' for available files. @ Saurabh1105/MMDet\n",
      "Checking model 108 optimum/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model optimum/segformer-b0-finetuned-ade-512-512 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ optimum/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 109 shi-labs/oneformer_coco_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ shi-labs/oneformer_coco_swin_large\n",
      "Checking model 110 facebook/maskformer-swin-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-large-ade\n",
      "Checking model 111 apple/deeplabv3-mobilevit-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.003245374993397, 256, 2] @ apple/deeplabv3-mobilevit-small\n",
      "Checking model 112 Plsek/CADET-v1\n",
      "#FITNESS ERROR: Plsek/CADET-v1 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Plsek/CADET-v1/main' for available files. @ Plsek/CADET-v1\n",
      "Checking model 113 Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: Robindboer/segformer-b5-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ Robindboer/segformer-b5-finetuned-segments-mopeds\n",
      "Checking model 114 facebook/mask2former-swin-tiny-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.000776291126176, 256, 2] @ facebook/mask2former-swin-tiny-ade-semantic\n",
      "Checking model 115 shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-small-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-small-youtubevis-2021-instance\n",
      "Checking model 116 facebook/mask2former-swin-large-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017874168321206, 256, 2] @ facebook/mask2former-swin-large-coco-panoptic\n",
      "Checking model 117 CIDAS/clipseg-rd16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd16\n",
      "Checking model 118 segments-tobias/segformer-b3-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ segments-tobias/segformer-b3-finetuned-segments-sidewalk\n",
      "Checking model 119 apple/mobilevitv2-1.0-voc-deeplabv3\n",
      "#FITNESS ERROR: Unrecognized image processor in apple/mobilevitv2-1.0-voc-deeplabv3. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, imagegpt, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos @ apple/mobilevitv2-1.0-voc-deeplabv3\n",
      "Checking model 120 nielsr/segformer-test-sidewalk-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-test-sidewalk-v2\n",
      "Checking model 121 facebook/maskformer-swin-small-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-small-coco\n",
      "Checking model 122 Intel/dpt-large-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ Intel/dpt-large-ade\n",
      "Checking model 123 mattmdjaga/segformer_b2_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0025724889216128, 256, 2] @ mattmdjaga/segformer_b2_clothes\n",
      "Checking model 124 malra/segformer-b5-segments-warehouse1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0029397036976007, 256, 2] @ malra/segformer-b5-segments-warehouse1\n",
      "Checking model 125 Matthijs/deeplabv3-mobilevit-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Matthijs/deeplabv3-mobilevit-small were not used when initializing MobileViTForSemanticSegmentation: ['mobilevit.encoder.layer.4.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.weight', 'seg_head.classifier.classifier.conv.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.layernorm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.conv.weight', 'seg_head.aspp.convs.2.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.num_batches_tracked', 'seg_head.aspp.project.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.weight', 'seg_head.aspp.convs.0.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.bias', 'seg_head.aspp.convs.1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.weight', 'seg_head.aspp.convs.1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.conv.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_before.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_mean', 'seg_head.aspp.convs.0.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.2.norm.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.bias', 'seg_head.aspp.convs.3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.fusion.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.0.intermediate.dense.weight', 'seg_head.aspp.convs.3.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.weight', 'mobilevit.encoder.layer.3.1.layernorm.weight', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.fusion.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.bias', 'seg_head.aspp.project.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.value.bias', 'mobilevit.conv_stem.norm.weight', 'mobilevit.encoder.layer.1.1.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.2.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.layernorm_after.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.bias', 'mobilevit.encoder.layer.4.1.fusion.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.output.dense.weight', 'mobilevit.conv_stem.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.running_mean', 'seg_head.aspp.convs.2.norm.running_mean', 'seg_head.aspp.project.norm.bias', 'seg_head.aspp.project.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.conv_kxk.conv.weight', 'seg_head.aspp.convs.0.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.key.bias', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.num_batches_tracked', 'seg_head.aspp.convs.1.norm.running_mean', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.bias', 'seg_head.aspp.convs.3.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.3.1.fusion.norm.bias', 'mobilevit.conv_stem.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.bias', 'seg_head.aspp.convs.3.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.conv.weight', 'seg_head.aspp.convs.0.norm.running_var', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.1.transformer.2.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_after.weight', 'seg_head.aspp.convs.3.norm.running_var', 'mobilevit.encoder.layer.3.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_before.weight', 'seg_head.aspp.project.norm.weight', 'mobilevit.encoder.layer.4.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.4.0.reduce_1x1.norm.running_mean', 'seg_head.aspp.project.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.1.transformer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.bias', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.4.0.conv_3x3.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.layernorm_before.weight', 'seg_head.aspp.convs.2.norm.bias', 'seg_head.aspp.convs.1.norm.running_var', 'mobilevit.encoder.layer.2.1.fusion.norm.weight', 'mobilevit.encoder.layer.3.1.conv_proj.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.layernorm.bias', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.3.output.dense.bias', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.num_batches_tracked', 'mobilevit.encoder.layer.2.1.conv_proj.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.layernorm_after.weight', 'mobilevit.encoder.layer.2.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.weight', 'seg_head.aspp.convs.0.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.4.1.transformer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.1.1.expand_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.1.output.dense.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.3.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.running_mean', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.3.1.fusion.norm.running_var', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.3.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.2.layernorm_after.bias', 'mobilevit.encoder.layer.2.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.1.1.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.0.output.dense.weight', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.weight', 'mobilevit.encoder.layer.4.0.reduce_1x1.conv.weight', 'seg_head.aspp.convs.1.norm.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.2.1.transformer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.4.1.conv_proj.norm.running_var', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.running_var', 'mobilevit.conv_stem.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.1.1.conv_3x3.norm.num_batches_tracked', 'seg_head.classifier.classifier.conv.weight', 'mobilevit.encoder.layer.1.1.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.4.1.transformer.0.layernorm_after.bias', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_var', 'mobilevit.encoder.layer.2.0.conv_3x3.norm.running_mean', 'mobilevit.encoder.layer.3.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.4.1.transformer.2.attention.output.dense.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.weight', 'mobilevit.encoder.layer.3.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.3.1.transformer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.2.1.transformer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.bias', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.running_mean', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.bias', 'mobilevit.encoder.layer.2.1.transformer.0.output.dense.bias', 'mobilevit.encoder.layer.3.1.transformer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.1.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.conv_proj.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.1.conv_proj.norm.running_var', 'mobilevit.conv_stem.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.transformer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.1.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.1.transformer.1.attention.attention.key.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.norm.running_var', 'seg_head.aspp.convs.2.conv.weight', 'mobilevit.encoder.layer.2.1.transformer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.norm.bias', 'mobilevit.encoder.layer.1.2.conv_3x3.norm.weight', 'seg_head.aspp.convs.1.norm.num_batches_tracked', 'mobilevit.encoder.layer.3.1.conv_kxk.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.norm.bias', 'mobilevit.encoder.layer.3.1.transformer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.1.layernorm.bias', 'mobilevit.encoder.layer.0.0.conv_3x3.norm.weight', 'mobilevit.encoder.layer.1.0.conv_3x3.conv.weight', 'seg_head.aspp.convs.4.aspp_pool.conv_1x1.conv.weight', 'mobilevit.encoder.layer.3.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.2.1.fusion.conv.weight', 'mobilevit.encoder.layer.1.0.expand_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.4.1.transformer.0.attention.output.dense.bias', 'seg_head.aspp.convs.0.conv.weight', 'mobilevit.encoder.layer.0.0.expand_1x1.conv.weight', 'mobilevit.encoder.layer.2.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.2.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.1.2.expand_1x1.norm.weight', 'seg_head.aspp.convs.2.norm.num_batches_tracked', 'mobilevit.encoder.layer.1.0.conv_3x3.norm.bias', 'mobilevit.encoder.layer.4.1.conv_kxk.norm.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.conv.weight', 'mobilevit.encoder.layer.4.0.conv_3x3.norm.running_var', 'mobilevit.encoder.layer.3.1.transformer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.0.0.reduce_1x1.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.2.1.fusion.norm.running_var', 'mobilevit.encoder.layer.4.1.transformer.0.attention.attention.key.weight', 'mobilevit.conv_stem.norm.running_mean', 'mobilevit.encoder.layer.3.1.transformer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.0.expand_1x1.norm.running_var', 'mobilevit.encoder.layer.1.2.reduce_1x1.norm.weight']\n",
      "- This IS expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileViTForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForSemanticSegmentation were not initialized from the model checkpoint at Matthijs/deeplabv3-mobilevit-small and are newly initialized: ['mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.convs.3.normalization.bias', 'mobilevit.encoder.layer.3.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.weight', 'segmentation_head.aspp.convs.2.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.2.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.2.fusion.normalization.weight', 'segmentation_head.aspp.convs.2.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.4.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_projection.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.fusion.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.convolution.weight', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.weight', 'mobilevit.encoder.layer.4.fusion.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.weight', 'mobilevit.encoder.layer.4.fusion.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.aspp.project.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.output.dense.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.weight', 'mobilevit.conv_stem.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.fusion.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.4.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.3.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.weight', 'mobilevit.conv_stem.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.bias', 'segmentation_head.aspp.convs.3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.bias', 'segmentation_head.aspp.convs.1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.weight', 'segmentation_head.aspp.convs.1.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.weight', 'segmentation_head.aspp.convs.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_var', 'segmentation_head.classifier.convolution.bias', 'segmentation_head.aspp.convs.0.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.bias', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.convolution.weight', 'mobilevit.conv_stem.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.conv_stem.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.bias', 'segmentation_head.aspp.convs.0.normalization.running_var', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.3.layernorm.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.2.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.weight', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_mean', 'segmentation_head.aspp.convs.1.normalization.running_var', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.weight', 'mobilevit.conv_stem.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.fusion.normalization.running_mean', 'segmentation_head.aspp.project.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.running_mean', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.conv_projection.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.3.fusion.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.weight', 'segmentation_head.classifier.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.convolution.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.1.output.dense.weight', 'segmentation_head.aspp.project.normalization.weight', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.running_var', 'segmentation_head.aspp.convs.2.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.weight', 'segmentation_head.aspp.convs.0.convolution.weight', 'mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.running_var', 'segmentation_head.aspp.convs.1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.project.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.bias', 'mobilevit.encoder.layer.3.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.0.normalization.bias', 'mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.4.layernorm.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.bias', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.weight', 'mobilevit.encoder.layer.3.conv_kxk.convolution.weight', 'mobilevit.encoder.layer.4.conv_projection.convolution.weight', 'mobilevit.encoder.layer.2.layernorm.bias', 'mobilevit.encoder.layer.2.conv_kxk.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.bias', 'mobilevit.encoder.layer.2.conv_projection.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.2.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.conv_kxk.normalization.num_batches_tracked', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.2.reduce_1x1.convolution.weight', 'segmentation_head.aspp.convs.1.normalization.bias', 'mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.weight', 'mobilevit.encoder.layer.4.conv_projection.normalization.num_batches_tracked', 'mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.weight', 'mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.bias', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.weight', 'mobilevit.encoder.layer.2.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.weight', 'mobilevit.encoder.layer.3.transformer.layer.2.output.dense.weight', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.weight', 'segmentation_head.aspp.convs.0.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.weight', 'segmentation_head.aspp.convs.0.normalization.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.bias', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.weight', 'mobilevit.encoder.layer.3.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.weight', 'mobilevit.encoder.layer.2.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.bias', 'segmentation_head.aspp.convs.2.normalization.weight', 'mobilevit.encoder.layer.1.layer.0.expand_1x1.convolution.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.weight', 'mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.bias', 'mobilevit.encoder.layer.4.fusion.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.bias', 'segmentation_head.aspp.convs.4.conv_1x1.normalization.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.running_mean', 'mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_kxk.normalization.num_batches_tracked', 'mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.bias', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.weight', 'mobilevit.encoder.layer.3.fusion.normalization.weight', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.bias', 'mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.running_mean', 'mobilevit.encoder.layer.3.fusion.normalization.num_batches_tracked', 'mobilevit.encoder.layer.4.layernorm.bias', 'segmentation_head.aspp.convs.3.convolution.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.output.dense.bias', 'mobilevit.conv_stem.normalization.bias', 'mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.2.conv_projection.normalization.running_var', 'segmentation_head.aspp.convs.3.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.running_mean', 'mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.num_batches_tracked', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.3.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.conv_1x1.convolution.weight', 'mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.running_mean', 'mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.bias', 'mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.weight', 'mobilevit.encoder.layer.3.conv_projection.normalization.bias', 'mobilevit.encoder.layer.1.layer.1.expand_1x1.convolution.weight', 'segmentation_head.aspp.project.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.conv_projection.normalization.weight', 'mobilevit.encoder.layer.3.fusion.normalization.running_var', 'mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.bias', 'mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.weight', 'mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.bias', 'segmentation_head.aspp.convs.2.normalization.bias', 'mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.bias', 'mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.num_batches_tracked', 'mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.4.conv_kxk.normalization.running_var', 'mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.bias', 'mobilevit.encoder.layer.4.conv_projection.normalization.weight', 'mobilevit.encoder.layer.2.fusion.normalization.bias', 'mobilevit.encoder.layer.4.transformer.layer.0.output.dense.weight', 'mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.weight', 'mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.running_var', 'mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.weight', 'mobilevit.encoder.layer.2.conv_kxk.convolution.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ Matthijs/deeplabv3-mobilevit-small\n",
      "Checking model 126 zoheb/mit-b5-finetuned-sidewalk-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ zoheb/mit-b5-finetuned-sidewalk-semantic\n",
      "Checking model 127 shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2019-instance\n",
      "Checking model 128 facebook/mask2former-swin-small-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-small-coco-instance\n",
      "Checking model 129 shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.0031804724609614, 256, 2] @ shaheen1998/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 130 skaliy/endometrical_cancer_segmentation\n",
      "#FITNESS ERROR: skaliy/endometrical_cancer_segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/skaliy/endometrical_cancer_segmentation/main' for available files. @ skaliy/endometrical_cancer_segmentation\n",
      "Checking model 131 Azarthehulk/Image_preprocessing_basics\n",
      "#FITNESS ERROR: Azarthehulk/Image_preprocessing_basics does not appear to have a file named config.json. Checkout 'https://huggingface.co/Azarthehulk/Image_preprocessing_basics/main' for available files. @ Azarthehulk/Image_preprocessing_basics\n",
      "Checking model 132 facebook/mask2former-swin-small-coco-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-small-coco-panoptic\n",
      "Checking model 133 facebook/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ facebook/detr-resnet-50-panoptic\n",
      "Checking model 134 shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-large-youtubevis-2019-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-large-youtubevis-2019-instance\n",
      "Checking model 135 s3nh/SegFormer-b5-person-segm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: s3nh/SegFormer-b5-person-segm does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/s3nh/SegFormer-b5-person-segm/main' for available files. @ s3nh/SegFormer-b5-person-segm\n",
      "Checking model 136 facebook/mask2former-swin-large-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ facebook/mask2former-swin-large-cityscapes-semantic\n",
      "Checking model 137 Robindboer/imageseg\n",
      "#FITNESS ERROR: Robindboer/imageseg does not appear to have a file named config.json. Checkout 'https://huggingface.co/Robindboer/imageseg/main' for available files. @ Robindboer/imageseg\n",
      "Checking model 138 facebook/mask2former-swin-base-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-base-ade-semantic\n",
      "Checking model 139 sayakpaul/mit-b0-finetuned-sidewalk-semantic\n",
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-sidewalk-semantic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-sidewalk-semantic\n",
      "Checking model 140 chainyo/segformer-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ chainyo/segformer-sidewalk\n",
      "Checking model 141 facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic were not used when initializing Mask2FormerForUniversalSegmentation: ['model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.model.layernorm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.0.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.3.bias', 'model.pixel_level_module.encoder.hidden_states_norms.3.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.embeddings.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.0.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.model.layernorm.bias', 'model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.model.embeddings.norm.bias', 'model.pixel_level_module.encoder.hidden_states_norms.1.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.2.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.model.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.model.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.model.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.model.encoder.layers.2.blocks.8.output.dense.bias']\n",
      "- This IS expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-cityscapes-semantic and are newly initialized: ['model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.embeddings.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage3.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight', 'model.pixel_level_module.encoder.hidden_states_norms.stage4.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.hidden_states_norms.stage2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-base-IN21k-cityscapes-semantic\n",
      "Checking model 142 thiagohersan/maskformer-satellite-trees\n",
      "#FITNESS ERROR: thiagohersan/maskformer-satellite-trees is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ thiagohersan/maskformer-satellite-trees\n",
      "Checking model 143 shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "#FITNESS ERROR: Could not load model shivi/video-mask2former-swin-tiny-youtubevis-2021-instance with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>). @ shivi/video-mask2former-swin-tiny-youtubevis-2021-instance\n",
      "Checking model 144 Efferbach/segformer-finetuned-lane-1k-steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ Efferbach/segformer-finetuned-lane-1k-steps\n",
      "Checking model 145 irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ irfan-noordin/segformer-b0-finetuned-segments-sidewalk-oct-22\n",
      "Checking model 146 chainyo/segformer-b1-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.003150459400593, 256, 2] @ chainyo/segformer-b1-sidewalk\n",
      "Checking model 147 facebook/mask2former-swin-base-IN21k-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-base-IN21k-cityscapes-instance\n",
      "Checking model 148 facebook/mask2former-swin-large-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ facebook/mask2former-swin-large-coco-instance\n",
      "Checking model 149 facebook/detr-resnet-101-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/detr-resnet-101-panoptic\n",
      "Checking model 150 nielsr/segformer-test-v5\n",
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-test-v5\n",
      "Checking model 151 openmmlab/upernet-convnext-tiny\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-convnext-tiny\n",
      "Checking model 152 nielsr/segformer-trainer-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-trainer-test\n",
      "Checking model 153 sayakpaul/mit-b0-finetuned-pets\n",
      "#FITNESS ERROR: Could not load model sayakpaul/mit-b0-finetuned-pets with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.segformer.modeling_segformer.SegformerForSemanticSegmentation'>). @ sayakpaul/mit-b0-finetuned-pets\n",
      "Checking model 154 thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ thesisabc/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 155 cvappbakkers/techniparts\n",
      "#FITNESS ERROR: cvappbakkers/techniparts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ cvappbakkers/techniparts\n",
      "Checking model 156 andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "#FITNESS [1.0030398536073153, 256, 2] @ andrewljohnson/segformer-b5-finetuned-magic-cards-230117-2\n",
      "Checking model 157 Abhilashvj/clipseg-rd64-refined-copy\n",
      "#FITNESS ERROR: Abhilashvj/clipseg-rd64-refined-copy does not appear to have a file named config.json. Checkout 'https://huggingface.co/Abhilashvj/clipseg-rd64-refined-copy/main' for available files. @ Abhilashvj/clipseg-rd64-refined-copy\n",
      "Checking model 158 Xpitfire/segformer-finetuned-segments-cmp-facade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ Xpitfire/segformer-finetuned-segments-cmp-facade\n",
      "Checking model 159 nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n",
      "Checking model 160 CIDAS/clipseg-rd64-refined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64-refined\n",
      "Checking model 161 alanoix/segformer_b0_flair_one\n",
      "#FITNESS ERROR: alanoix/segformer_b0_flair_one does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/alanoix/segformer_b0_flair_one/main' for available files. @ alanoix/segformer_b0_flair_one\n",
      "Checking model 162 bilal01/segformer-b0-finetuned-segments-test\n",
      "#FITNESS [1.0030985175029126, 256, 2] @ bilal01/segformer-b0-finetuned-segments-test\n",
      "Checking model 163 Xenova/detr-resnet-50-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Could not load model Xenova/detr-resnet-50-panoptic with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageSegmentation'>, <class 'transformers.models.auto.modeling_auto.AutoModelForSemanticSegmentation'>, <class 'transformers.models.detr.modeling_detr.DetrForSegmentation'>). @ Xenova/detr-resnet-50-panoptic\n",
      "Checking model 164 keremberke/yolov8m-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8m-pcb-defect-segmentation\n",
      "Checking model 165 Efferbach/mobilenet_v2_1-10k-steps\n",
      "#FITNESS [1.5, 256, 1] @ Efferbach/mobilenet_v2_1-10k-steps\n",
      "Checking model 166 jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b3-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b3-finetuned-segments-mopeds\n",
      "Checking model 167 matjesg/cFOS_in_HC\n",
      "#FITNESS ERROR: matjesg/cFOS_in_HC does not appear to have a file named config.json. Checkout 'https://huggingface.co/matjesg/cFOS_in_HC/main' for available files. @ matjesg/cFOS_in_HC\n",
      "Checking model 168 fcakyon/test-model\n",
      "#FITNESS ERROR: 'v8' @ fcakyon/test-model\n",
      "Checking model 169 androks/rembg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: androks/rembg does not appear to have a file named config.json. Checkout 'https://huggingface.co/androks/rembg/main' for available files. @ androks/rembg\n",
      "Checking model 170 facebook/mask2former-swin-tiny-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ facebook/mask2former-swin-tiny-cityscapes-panoptic\n",
      "Checking model 171 openmmlab/upernet-convnext-base\n",
      "#FITNESS [1.5, 256, 2] @ openmmlab/upernet-convnext-base\n",
      "Checking model 172 segments-tobias/segformer-b0-finetuned-segments-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ segments-tobias/segformer-b0-finetuned-segments-sidewalk\n",
      "Checking model 173 q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "#FITNESS [1.5, 256, 1] @ q2-jlbar/segformer-b0-finetuned-brooks-or-dunn\n",
      "Checking model 174 nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b3-finetuned-cityscapes-1024-1024\n",
      "Checking model 175 jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "#FITNESS ERROR: jordibeen/segformer-b0-finetuned-segments-mopeds is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ jordibeen/segformer-b0-finetuned-segments-mopeds\n",
      "Checking model 176 Narsil/pet-segmentation\n",
      "#FITNESS ERROR: Unrecognized model in Narsil/pet-segmentation. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso @ Narsil/pet-segmentation\n",
      "Checking model 177 jakka/segformer-b0-finetuned-warehouse-part-1-V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.003244871926085, 256, 2] @ jakka/segformer-b0-finetuned-warehouse-part-1-V2\n",
      "Checking model 178 facebook/maskformer-swin-tiny-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-tiny-coco\n",
      "Checking model 179 Mendel192/san\n",
      "#FITNESS ERROR: Mendel192/san does not appear to have a file named config.json. Checkout 'https://huggingface.co/Mendel192/san/main' for available files. @ Mendel192/san\n",
      "Checking model 180 facebook/mask2former-swin-small-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ facebook/mask2former-swin-small-cityscapes-panoptic\n",
      "Checking model 181 facebook/mask2former-swin-tiny-coco-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0032476210038814, 256, 2] @ facebook/mask2former-swin-tiny-coco-instance\n",
      "Checking model 182 nvidia/segformer-b5-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0021701880290503, 256, 2] @ nvidia/segformer-b5-finetuned-ade-640-640\n",
      "Checking model 183 openmmlab/upernet-swin-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0007391988163103, 256, 2] @ openmmlab/upernet-swin-small\n",
      "Checking model 184 shi-labs/oneformer_cityscapes_dinat_large\n",
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_cityscapes_dinat_large\n",
      "Checking model 185 keras-io/monocular-depth-estimation\n",
      "#FITNESS ERROR: keras-io/monocular-depth-estimation does not appear to have a file named config.json. Checkout 'https://huggingface.co/keras-io/monocular-depth-estimation/main' for available files. @ keras-io/monocular-depth-estimation\n",
      "Checking model 186 mraottth/trashbot_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ mraottth/trashbot_v1\n",
      "Checking model 187 yahaoh/ddh-maskrcnn\n",
      "#FITNESS ERROR: yahaoh/ddh-maskrcnn does not appear to have a file named config.json. Checkout 'https://huggingface.co/yahaoh/ddh-maskrcnn/main' for available files. @ yahaoh/ddh-maskrcnn\n",
      "Checking model 188 shi-labs/oneformer_coco_dinat_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: \n",
      "DinatBackbone requires the natten library but it was not found in your environment. You can install it by referring to:\n",
      "shi-labs.com/natten . You can also install it with pip (may take longer to build):\n",
      "`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
      " @ shi-labs/oneformer_coco_dinat_large\n",
      "Checking model 189 nielsr/segformer-finetuned-sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nielsr/segformer-finetuned-sidewalk\n",
      "Checking model 190 shi-labs/oneformer_cityscapes_swin_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ shi-labs/oneformer_cityscapes_swin_large\n",
      "Checking model 191 reannayang/segformer-b0-pavement\n",
      "#FITNESS [1.5, 256, 2] @ reannayang/segformer-b0-pavement\n",
      "Checking model 192 maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "#FITNESS ERROR: maryann-gitonga/brain-tumor-segmentation-3d-attention-unet does not appear to have a file named config.json. Checkout 'https://huggingface.co/maryann-gitonga/brain-tumor-segmentation-3d-attention-unet/main' for available files. @ maryann-gitonga/brain-tumor-segmentation-3d-attention-unet\n",
      "Checking model 193 nvidia/segformer-b0-finetuned-ade-512-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b0-finetuned-ade-512-512\n",
      "Checking model 194 jonathandinu/face-parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0011785247279477, 256, 2] @ jonathandinu/face-parsing\n",
      "Checking model 195 keremberke/yolov8n-pothole-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8n-pothole-segmentation\n",
      "Checking model 196 ksahn/pid-s\n",
      "#FITNESS ERROR: ksahn/pid-s is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`. @ ksahn/pid-s\n",
      "Checking model 197 apple/deeplabv3-mobilevit-xx-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0032549033337128, 256, 2] @ apple/deeplabv3-mobilevit-xx-small\n",
      "Checking model 198 nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b4-finetuned-cityscapes-1024-1024\n",
      "Checking model 199 facebook/mask2former-swin-small-cityscapes-instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: list index out of range @ facebook/mask2former-swin-small-cityscapes-instance\n",
      "Checking model 200 facebook/detr-resnet-50-dc5-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.00179973055557, 256, 2] @ facebook/detr-resnet-50-dc5-panoptic\n",
      "Checking model 201 keja/deeplab-v3\n",
      "#FITNESS ERROR: keja/deeplab-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/keja/deeplab-v3/main' for available files. @ keja/deeplab-v3\n",
      "Checking model 202 Sevenlee/kkk\n",
      "#FITNESS ERROR: Sevenlee/kkk does not appear to have a file named config.json. Checkout 'https://huggingface.co/Sevenlee/kkk/main' for available files. @ Sevenlee/kkk\n",
      "Checking model 203 jakka/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ jakka/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 204 Niceforo/teste\n",
      "#FITNESS ERROR: Niceforo/teste does not appear to have a file named config.json. Checkout 'https://huggingface.co/Niceforo/teste/main' for available files. @ Niceforo/teste\n",
      "Checking model 205 mattmdjaga/segformer_b0_clothes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0015715639257186, 256, 2] @ mattmdjaga/segformer_b0_clothes\n",
      "Checking model 206 cosmobaby/ka\n",
      "#FITNESS ERROR: cosmobaby/ka does not appear to have a file named config.json. Checkout 'https://huggingface.co/cosmobaby/ka/main' for available files. @ cosmobaby/ka\n",
      "Checking model 207 CIDAS/clipseg-rd64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The model 'CLIPSegForImageSegmentation' is not supported for image-segmentation. Supported models are ['DetrForSegmentation', 'BeitForSemanticSegmentation', 'Data2VecVisionForSemanticSegmentation', 'DPTForSemanticSegmentation', 'MobileNetV2ForSemanticSegmentation', 'MobileViTForSemanticSegmentation', 'MobileViTV2ForSemanticSegmentation', 'SegformerForSemanticSegmentation', 'UperNetForSemanticSegmentation', 'MaskFormerForInstanceSegmentation', 'Mask2FormerForUniversalSegmentation', 'OneFormerForUniversalSegmentation'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS ERROR: Invalid conditional, should be either provided as `input_ids` or `conditional_pixel_values` @ CIDAS/clipseg-rd64\n",
      "Checking model 208 voitl/unet_plus_plus\n",
      "#FITNESS ERROR: No module named 'segmentation_models_pytorch' @ voitl/unet_plus_plus\n",
      "Checking model 209 facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0017827173582279, 256, 2] @ facebook/mask2former-swin-base-IN21k-cityscapes-panoptic\n",
      "Checking model 210 Lewislou/cellseg_sribd\n",
      "#FITNESS ERROR: 'cell_sribd' @ Lewislou/cellseg_sribd\n",
      "Checking model 211 malra/segformer-b0-finetuned-segments-sidewalk-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ malra/segformer-b0-finetuned-segments-sidewalk-4\n",
      "Checking model 212 facebook/mask2former-swin-large-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-large-ade-semantic\n",
      "Checking model 213 keremberke/yolov8s-building-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-building-segmentation\n",
      "Checking model 214 nishita/segformer-b0-finetuned-segments-sidewalk-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nishita/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 215 plant/segformer-b5-finetuned-segments-instryde-foot-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 1] @ plant/segformer-b5-finetuned-segments-instryde-foot-test\n",
      "Checking model 216 userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "#FITNESS [1.0031732898728796, 256, 2] @ userGagan/segformer-b0-finetuned-segments-sidewalk-2\n",
      "Checking model 217 openmmlab/upernet-swin-tiny\n",
      "#FITNESS [1.0018509113570069, 256, 2] @ openmmlab/upernet-swin-tiny\n",
      "Checking model 218 nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n",
      "Checking model 219 rawjaw/metastore-segmentation\n",
      "#FITNESS ERROR: rawjaw/metastore-segmentation does not appear to have a file named config.json. Checkout 'https://huggingface.co/rawjaw/metastore-segmentation/main' for available files. @ rawjaw/metastore-segmentation\n",
      "Checking model 220 keremberke/yolov8s-pcb-defect-segmentation\n",
      "#FITNESS ERROR: 'v8' @ keremberke/yolov8s-pcb-defect-segmentation\n",
      "Checking model 221 facebook/mask2former-swin-small-ade-semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/mask2former-swin-small-ade-semantic\n",
      "Checking model 222 microsoft/beit-large-finetuned-ade-640-640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ microsoft/beit-large-finetuned-ade-640-640\n",
      "Checking model 223 facebook/maskformer-swin-small-ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-small-ade\n",
      "Checking model 224 kasumi222/segformer-b0-finetuned-busigt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.0028116413133186, 256, 2] @ kasumi222/segformer-b0-finetuned-busigt2\n",
      "Checking model 225 facebook/maskformer-swin-base-coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "`label_ids_to_fuse` unset. No instance will be fused.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FITNESS [1.5, 256, 2] @ facebook/maskformer-swin-base-coco\n",
      "Checking model 226 matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "#FITNESS [1.0026279211457534, 256, 2] @ matei-dorian/segformer-b5-finetuned-human-parsing\n",
      "Checking model 227 lapix/segformer-b3-finetuned-ccagt-400-300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "for image_name,GT_name in image_names:\n",
    "    img = Image.open(image_name).convert(\"RGB\")\n",
    "    GT = np.array(Image.open(GT_name).convert(\"RGB\"))[:,:,0]\n",
    "    results = list_of_models.checkall(img, GT, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b210d6-f9bd-4d00-86c2-3b055955bd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
