{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f9cda4",
   "metadata": {},
   "source": [
    "# Using Pipeline class from Transformers Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SEEhug as hug\n",
    "\n",
    "#im = imread('myimage.png')\n",
    "results = hug.allcheck(10)\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5c7746",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modelfile.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_of_models\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlist_of_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hugging_face/wrapper/list_of_models.py:59\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(filename)\n\u001b[1;32m     58\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     60\u001b[0m     model_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     61\u001b[0m     models\u001b[38;5;241m.\u001b[39mextend(model_list)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modelfile.txt'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(hug.allcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb273b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper import list_of_models\n",
    "model = list_of_models.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model)\n",
    "# models = models[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df7cf3a",
   "metadata": {},
   "source": [
    "#### Extract all the images from our dataset and divide them into ground truths and unsegmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1567c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagepath = pathlib.Path('./Images/')\n",
    "images = imagepath.glob('*')\n",
    "inferred = []\n",
    "GT = []\n",
    "for image in images:\n",
    "    if \"GT\" in image.name:\n",
    "        GT.append(image)\n",
    "\n",
    "    elif \"GT\" not in image.name and \"ipy\" not in image.name:\n",
    "        inferred.append(image)\n",
    "GT_paths = sorted(GT)\n",
    "input_paths = sorted(inferred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1a00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Test the generated mask with ground truth using SEE-Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6855d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ishasharma/Desktop/SEE Git/see-segment/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94e15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from see import Segment_Fitness as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1e2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitness_table = []\n",
    "for image_path, GT_path in zip(input_paths, GT_paths):\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # NOTE TO DIRK, why does adding [:,:,0] change the fitness?\n",
    "    GT_image = np.array(Image.open(GT_path).convert(\"RGB\"))[:,:,0]\n",
    "    \n",
    "    # Create subplots for each model's segmented image\n",
    "    fig, axs = plt.subplots(1, len(models))\n",
    "    \n",
    "    fitness = []\n",
    "    # Iterate over the models and display segmented images\n",
    "    \n",
    "    for i, model_name in enumerate(models):\n",
    "        # Initialize an image segmentation pipeline\n",
    "        try:\n",
    "            segmentation_pipeline = pipeline(\"image-segmentation\", model=model_name)\n",
    "            outputs = segmentation_pipeline(image)\n",
    "            inferred_segmentation = outputs[0]['mask']\n",
    "\n",
    "            # Display the segmented image in the corresponding subplot\n",
    "            ax = axs[i] if len(models) > 1 else axs\n",
    "            ax.imshow(inferred_segmentation)\n",
    "            fit = sf.FitnessFunction(np.array(inferred_segmentation), GT_image)\n",
    "            fitness.append(fit[0])\n",
    "        except:\n",
    "            fitness.append(\"NaN\")\n",
    "            print(\"Gave error\")\n",
    "    fitness_table.append(fitness)\n",
    "\n",
    "        # Adjust the layout to avoid overlapping images\n",
    "    fig.tight_layout()\n",
    "\n",
    "        # Show the figure for the current image\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a86a2e-485d-42fb-b4ed-035483d3959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ac772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(fitness_table, index = inferred, columns = models)\n",
    "df.index.names = [\"Images\"]\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
